#!/usr/bin/env python3
"""
MCP RAG Server - ìœ„í‚¤í”¼ë””ì•„ ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ ë‹µë³€ ì‹œìŠ¤í…œ
ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ì í•©í•œ íŒŒì¼ì„ ì°¾ê³  ê´€ë ¨ ë‚´ìš©ì„ ì¶”ì¶œí•˜ëŠ” MCP ì„œë²„
"""

import os
import logging
from pathlib import Path
from typing import Any, List, Dict
import re
from fastmcp import FastMCP

# ë¡œê¹… ì„¤ì • (stderrë¡œ ì¶œë ¥ - MCP ì„œë²„ì—ì„œëŠ” stdout ì‚¬ìš© ê¸ˆì§€)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# FastMCP ì„œë²„ ì´ˆê¸°í™”
mcp = FastMCP("rag-server")

# ìƒìˆ˜ ì„¤ì •
DATA_DIR = Path("../data/raw")
SUPPORTED_EXTENSIONS = {".md", ".txt"}

def get_all_documents() -> List[Dict[str, Any]]:
    """
    data/raw í´ë”ì—ì„œ ëª¨ë“  ë¬¸ì„œ íŒŒì¼ ì •ë³´ë¥¼ ê°€ì ¸ì˜´
    
    Returns:
        ë¬¸ì„œ ì •ë³´ ë¦¬ìŠ¤íŠ¸ [{"title": str, "path": Path, "size": int}, ...]
    """
    documents = []
    
    if not DATA_DIR.exists():
        logger.warning(f"ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {DATA_DIR}")
        return documents
    
    for file_path in DATA_DIR.rglob("*"):
        if file_path.is_file() and file_path.suffix.lower() in SUPPORTED_EXTENSIONS:
            title = file_path.stem  # í™•ì¥ì ì œì™¸í•œ íŒŒì¼ëª…
            documents.append({
                "title": title,
                "path": file_path,
                "size": file_path.stat().st_size
            })
    
    logger.info(f"ì´ {len(documents)}ê°œ ë¬¸ì„œë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
    return documents

def calculate_relevance_score(query: str, title: str) -> float:
    """
    ì§ˆë¬¸ê³¼ íŒŒì¼ ì œëª© ê°„ì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
    
    Args:
        query: ì‚¬ìš©ì ì§ˆë¬¸
        title: íŒŒì¼ ì œëª©
        
    Returns:
        ê´€ë ¨ì„± ì ìˆ˜ (0.0 ~ 1.0)
    """
    query_lower = query.lower()
    title_lower = title.lower()
    
    score = 0.0
    
    # ì •í™•í•œ ë§¤ì¹­ (ê°€ì¥ ë†’ì€ ì ìˆ˜)
    if title_lower in query_lower or query_lower in title_lower:
        score += 0.8
    
    # ë‹¨ì–´ë³„ ë§¤ì¹­
    query_words = set(re.findall(r'\b\w+\b', query_lower))
    title_words = set(re.findall(r'\b\w+\b', title_lower))
    
    if query_words and title_words:
        common_words = query_words.intersection(title_words)
        word_score = len(common_words) / max(len(query_words), len(title_words))
        score += word_score * 0.6
    
    # í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤ì¹­ (AI/ML ê´€ë ¨)
    ai_keywords = {
        'ai', 'artificial', 'intelligence', 'ì¸ê³µì§€ëŠ¥', 'ai',
        'machine', 'learning', 'ë¨¸ì‹ ëŸ¬ë‹', 'ê¸°ê³„í•™ìŠµ', 'ml',
        'deep', 'deeplearning', 'ë”¥ëŸ¬ë‹', 'neural', 'network', 'ì‹ ê²½ë§',
        'cnn', 'rnn', 'lstm', 'gru', 'transformer', 'gpt',
        'reinforcement', 'ê°•í™”í•™ìŠµ', 'alphago', 'ì•ŒíŒŒê³ '
    }
    
    query_ai_words = set(query_lower.split()) & ai_keywords
    title_ai_words = set(title_lower.split()) & ai_keywords
    
    if query_ai_words and title_ai_words:
        score += 0.3
    
    return min(score, 1.0)

def extract_relevant_content(file_path: Path, query: str, max_lines: int = 50) -> str:
    """
    íŒŒì¼ì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì„ ì¶”ì¶œ
    
    Args:
        file_path: íŒŒì¼ ê²½ë¡œ
        query: ì‚¬ìš©ì ì§ˆë¬¸
        max_lines: ìµœëŒ€ ì¶”ì¶œí•  ì¤„ ìˆ˜
        
    Returns:
        ê´€ë ¨ ë‚´ìš© í…ìŠ¤íŠ¸
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # YAML ë©”íƒ€ë°ì´í„° ì œê±°
        if content.startswith('---'):
            parts = content.split('---', 2)
            if len(parts) >= 3:
                content = parts[2].strip()
        
        lines = content.split('\n')
        query_lower = query.lower()
        relevant_lines = []
        
        # ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì„¹ì…˜ ì°¾ê¸°
        for i, line in enumerate(lines):
            line_lower = line.lower()
            
            # í—¤ë”ë‚˜ ì¤‘ìš”í•œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì¤„ ìš°ì„  ì„ íƒ
            if any(keyword in line_lower for keyword in query_lower.split()):
                # í•´ë‹¹ ì¤„ê³¼ ì£¼ë³€ ë§¥ë½ í¬í•¨
                start = max(0, i - 2)
                end = min(len(lines), i + 8)
                context = lines[start:end]
                relevant_lines.extend(context)
                
                if len(relevant_lines) >= max_lines:
                    break
        
        # ê´€ë ¨ ë‚´ìš©ì´ ì—†ìœ¼ë©´ íŒŒì¼ ì‹œì‘ ë¶€ë¶„ ë°˜í™˜
        if not relevant_lines:
            relevant_lines = lines[:max_lines]
        
        return '\n'.join(relevant_lines[:max_lines])
        
    except Exception as e:
        logger.error(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ ({file_path}): {e}")
        return f"íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}"

@mcp.tool()
async def search_files(query: str, max_results: int = 3) -> str:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ íŒŒì¼ì œëª©ì„ ë³´ê³  ì í•©í•œ íŒŒì¼ë“¤ì„ ì„ íƒ
    
    Args:
        query: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ë‚˜ ê²€ìƒ‰ì–´
        max_results: ë°˜í™˜í•  ìµœëŒ€ íŒŒì¼ ìˆ˜ (ê¸°ë³¸ê°’: 3)
    """
    logger.info(f"íŒŒì¼ ê²€ìƒ‰ ìš”ì²­: '{query}'")
    
    documents = get_all_documents()
    
    if not documents:
        return "ê²€ìƒ‰ ê°€ëŠ¥í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. data/raw í´ë”ì— ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”."
    
    # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° ë° ì •ë ¬
    scored_docs = []
    for doc in documents:
        score = calculate_relevance_score(query, doc["title"])
        if score > 0.1:  # ìµœì†Œ ê´€ë ¨ì„± ì„ê³„ê°’
            scored_docs.append((score, doc))
    
    # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
    scored_docs.sort(key=lambda x: x[0], reverse=True)
    
    if not scored_docs:
        return f"'{query}'ì™€ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”."
    
    # ìµœëŒ€ ê²°ê³¼ ìˆ˜ë§Œí¼ ë°˜í™˜
    result_docs = scored_docs[:max_results]
    
    result = f"'{query}'ì— ëŒ€í•´ {len(result_docs)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤:\n\n"
    
    for i, (score, doc) in enumerate(result_docs, 1):
        result += f"{i}. **{doc['title']}**\n"
        result += f"   - ê´€ë ¨ì„±: {score:.2f}\n"
        result += f"   - íŒŒì¼ í¬ê¸°: {doc['size']} bytes\n"
        result += f"   - ê²½ë¡œ: {doc['path']}\n\n"
    
    result += "ğŸ’¡ ì´ íŒŒì¼ë“¤ì˜ ë‚´ìš©ì„ ë³´ë ¤ë©´ `get_relevant_content` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
    
    return result

@mcp.tool()
async def get_relevant_content(file_title: str, query: str, max_lines: int = 50) -> str:
    """
    íŠ¹ì • íŒŒì¼ì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì„ ì¶”ì¶œ
    
    Args:
        file_title: íŒŒì¼ ì œëª© (í™•ì¥ì ì œì™¸)
        query: ì‚¬ìš©ìì˜ ì§ˆë¬¸
        max_lines: ì¶”ì¶œí•  ìµœëŒ€ ì¤„ ìˆ˜ (ê¸°ë³¸ê°’: 50)
    """
    logger.info(f"ë‚´ìš© ì¶”ì¶œ ìš”ì²­: '{file_title}' íŒŒì¼ì—ì„œ '{query}' ê´€ë ¨ ë‚´ìš©")
    
    documents = get_all_documents()
    
    # íŒŒì¼ ì œëª©ìœ¼ë¡œ ë¬¸ì„œ ì°¾ê¸°
    target_file = None
    for doc in documents:
        if doc["title"].lower() == file_title.lower():
            target_file = doc["path"]
            break
    
    if not target_file:
        available_titles = [doc["title"] for doc in documents]
        return f"'{file_title}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì¼ë“¤: {', '.join(available_titles[:10])}"
    
    # ê´€ë ¨ ë‚´ìš© ì¶”ì¶œ
    content = extract_relevant_content(target_file, query, max_lines)
    
    result = f"## {file_title}\n"
    result += f"**ì§ˆë¬¸:** {query}\n\n"
    result += f"**ê´€ë ¨ ë‚´ìš©:**\n\n{content}\n\n"
    result += f"ğŸ“„ íŒŒì¼ ê²½ë¡œ: {target_file}"
    
    return result

if __name__ == "__main__":
    logger.info("MCP RAG ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # ë°ì´í„° ë””ë ‰í† ë¦¬ í™•ì¸
    if not DATA_DIR.exists():
        logger.warning(f"ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {DATA_DIR}")
        logger.info("../data/raw/ í´ë”ë¥¼ ìƒì„±í•˜ê³  ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
    
    # FastMCP ì„œë²„ ì‹¤í–‰
    mcp.run()