---
title: 강화 학습
type: 위키피디아 문서
format: markdown
---

# 강화 학습

- 신뢰성 있고 확인할 수 있는 출처가 제시되도록 번역하여 주십시오.
- 번역을 완료한 후에는 {{번역된 문서}} 틀을 토론창에 표기하여 저작자를 표시하여 주십시오.
- 문맥상 이해를 돕기 위해 관련 문서를 같이 번역해주시는 것이 좋습니다.
- 번역을 확장할 필요가 있는 내용이 포함된 다른 문서를 보고 싶으시다면 분류:번역 확장 필요 문서를 참고해주세요.

- 지도 학습
- 비지도 학습
- 온라인 기계 학습
- 메타-학습
- 준지도 학습
- 자기 지도 학습
- 강화 학습
- 규칙 기반 기계 학습
- 뉴로모픽 엔지니어링
- 양자 기계 학습

- 분류
- 생성 모델
- 회귀 분석
- 클러스터 분석
- 차원 축
- 이상 탐지
- 데이터 정제
- AutoML
- 연관 규칙 학습
- 구조 기반 예측
- 특징 공학
- 특징 학습
- 순위 학습
- 문법 유도
- 온톨로지 학습
- 멀티모달 학습

- 결정 트리 학습법
- 앙상블 학습법 배깅 부스팅 랜덤 포레스트
- 최근접 이웃 탐색
- k-NN
- 선형 회귀
- 나이브 베이즈
- 인공신경망
- 로지스틱 회귀
- 퍼셉트론
- 상관 벡터 머신(RVM)
- 서포트 벡터 머신(SVM)

- 배깅
- 부스팅
- 랜덤 포레스트

- BIRCH
- CURE 알고리즘
- 계층적 군집화
- k-평균 알고리즘
- 퍼지 클러스터링
- 기댓값 최대화 알고리즘
- DBSCAN
- OPTICS
- 평균이동

- 인자 분석
- CCA
- 독립 성분 분석
- 선형 판별 분석
- 음수 미포함 행렬 분해
- 주성분 분석
- t-SNE

- 그래프 모형 베이즈 네트워크 조건부 무작위장 은닉 마르코프 모형
- 잠재 디리클레 할당

- 베이즈 네트워크
- 조건부 무작위장
- 은닉 마르코프 모형

- 무작위 표본 합의
- k-최근접 이웃 알고리즘
- 국소 특이점 요인
- 고립 포레스트

- 오토인코더
- 딥 러닝
- 순방향 신경망
- 순환 신경망 LSTM GRU
- 볼츠만 머신 제한된
- 생성적 적대 신경망
- 확산 모델
- 자기조직화 지도
- 합성곱 신경망 U-Net LeNet 알렉스넷 딥드림
- 신경장 신경 방사장 물리정보 신경망
- 트랜스포머 비전
- 맘바
- 스파이킹 신경망
- 멤트렌지스터
- 전기화학 RAM
- 다층 퍼셉트론

- LSTM
- GRU

- 제한된

- U-Net
- LeNet
- 알렉스넷
- 딥드림

- 신경 방사장
- 물리정보 신경망

- 비전

- Q 러닝
- SARSA
- 시간차 학습

- 액티브 러닝
- 크라우드소싱
- 휴먼인더루프
- RLHF

- 결정계수
- 혼동 행렬
- 러닝 커브
- 수신자 조작 특성

- 커널 메소드
- 편향-분산 트레이드오프
- 계산학습이론
- 경험적 위험 최소화
- PAC 러닝
- 통계적 학습이론
- VC 이론

- NeurIPS
- ICML
- ICLR
- ML
- JMLR

- 기계 학습 알고리즘 목록
- 기계 탈학습
- 지식 증류
- 유사도 학습
- 대조 학습

- v
- t
- e

**강화 학습**(reinforcement learning)은 [기계 학습](https://ko.wikipedia.org/wiki/%EA%B8%B0%EA%B3%84_%ED%95%99%EC%8A%B5)의 한 영역이다. 행동심리학에서 영감을 받았으며, 어떤 환경 안에서 정의된 에이전트가 현재의 상태를 인식하여, 선택 가능한 행동들 중 보상을 최대화하는 행동 혹은 행동 순서를 선택하는 방법이다. 이러한 문제는 매우 포괄적이기 때문에 [게임 이론](https://ko.wikipedia.org/wiki/%EA%B2%8C%EC%9E%84_%EC%9D%B4%EB%A1%A0), [제어이론](https://ko.wikipedia.org/wiki/%EC%A0%9C%EC%96%B4%EC%9D%B4%EB%A1%A0), [운용 과학](https://ko.wikipedia.org/wiki/%EC%9A%B4%EC%9A%A9_%EA%B3%BC%ED%95%99), [정보이론](https://ko.wikipedia.org/wiki/%EC%A0%95%EB%B3%B4%EC%9D%B4%EB%A1%A0), 시뮬레이션 기반 [최적화](https://ko.wikipedia.org/wiki/%EC%B5%9C%EC%A0%81%ED%99%94_%EB%AC%B8%EC%A0%9C), [다중 에이전트 시스템](https://ko.wikipedia.org/wiki/%EB%8B%A4%EC%A4%91_%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8_%EC%8B%9C%EC%8A%A4%ED%85%9C), [떼 지능](https://ko.wikipedia.org/wiki/%EB%96%BC_%EC%A7%80%EB%8A%A5), [통계학](https://ko.wikipedia.org/wiki/%ED%86%B5%EA%B3%84%ED%95%99), [유전 알고리즘](https://ko.wikipedia.org/wiki/%EC%9C%A0%EC%A0%84_%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98) 등의 분야에서도 연구된다. 운용 과학과 제어 이론에서 강화 학습이 연구되는 분야는 "근사 동적 계획법"이라고 불린다. 또한 최적화 제어 이론에서도 유사한 문제를 연구하지만, 대부분의 연구가 최적해의 존재와 특성에 초점을 맞춘다는 점에서 학습과 근사의 측면에서 접근하는 강화 학습과는 다르다. [경제학](https://ko.wikipedia.org/wiki/%EA%B2%BD%EC%A0%9C%ED%95%99)과 게임 이론 분야에서 강화 학습은 어떻게 [제한된 합리성](https://ko.wikipedia.org/wiki/%EC%A0%9C%ED%95%9C%EB%90%9C_%ED%95%A9%EB%A6%AC%EC%84%B1) 하에서 평형이 일어날 수 있는지를 설명하는 데에 사용되기도 한다.

강화 학습에서 다루는 '환경'은 주로 [마르코프 결정 과정](https://ko.wikipedia.org/wiki/%EB%A7%88%EB%A5%B4%EC%BD%94%ED%94%84_%EA%B2%B0%EC%A0%95_%EA%B3%BC%EC%A0%95)으로 주어진다. 마르코프 결정 과정 문제를 해결하는 기존의 방식과 강화 학습이 다른 지점은, 강화 학습은 마르코프 결정 과정에 대한 지식을 요구하지 않는다는 점과, 강화 학습은 크기가 매우 커서 결정론적 방법을 적용할 수 없는 규모의 마르코프 결정 과정 문제를 다룬다는 점이다.

강화 학습은 또한 입출력 쌍으로 이루어진 훈련 집합이 제시되지 않으며, 잘못된 행동에 대해서도 명시적으로 정정이 일어나지 않는다는 점에서 일반적인 [지도 학습](https://ko.wikipedia.org/wiki/%EC%A7%80%EB%8F%84_%ED%95%99%EC%8A%B5)과 다르다. 대신, 강화학습의 초점은 학습 과정에서의(on-line) 성능이며, 이는 탐색(exploration)과 이용(exploitation)의 균형을 맞춤으로써 제고된다. 탐색과 이용의 균형 문제 강화 학습에서 가장 많이 연구된 문제로, [멀티 암드 밴딧](https://ko.wikipedia.org/wiki/%EB%A9%80%ED%8B%B0_%EC%95%94%EB%93%9C_%EB%B0%B4%EB%94%A7)와 유한한 마르코프 결정 과정 등에서 연구되었다.

### 개요

기본적으로 강화 학습의 문제는 [마르코프 결정 과정](https://ko.wikipedia.org/wiki/%EB%A7%88%EB%A5%B4%EC%BD%94%ED%94%84_%EA%B2%B0%EC%A0%95_%EA%B3%BC%EC%A0%95)(MDP)으로 표현된다. 이런 관점에서 강화 학습 알고리즘은 [동적 계획법](https://ko.wikipedia.org/wiki/%EB%8F%99%EC%A0%81_%EA%B3%84%ED%9A%8D%EB%B2%95)과 깊은 연관이 있다. 마르코프 결정 과정에서 상태 전이 확률(state transition probabilities)과 보상은 확률에 따른 값일 수도 있고, 이미 결정되어 있는 값일 수도 있다.

강화 학습이 원하지 않는 행동을 명시적으로 수정하는 [지도 학습](https://ko.wikipedia.org/wiki/%EC%A7%80%EB%8F%84_%ED%95%99%EC%8A%B5)과 다른 점은 온라인 수행에 중심을 두고 있다는 점이다. 강화 학습은 아직 조사되지 않는 영역을 탐험하는 것과 이미 알고 있는 지식을 이용하는 것의 균형을 잡는 것이다. 이 탐험과 이용 사이에 있는 트레이드오프는 [멀티 암드 밴딧](https://ko.wikipedia.org/wiki/%EB%A9%80%ED%8B%B0_%EC%95%94%EB%93%9C_%EB%B0%B4%EB%94%A7)과 같은 문제에서 알아 볼 수 있다.

### 알고리즘

수식으로 표현하면, 강화 학습 모델은 다음과 같이 구성된다.

1. 환경 상태 집합, $\displaystyle S}$;
2. 행동 집합, $\displaystyle A}$;
3. 포상($\displaystyle \in \mathbb {R} }$)의 집합;.

매 시점 $\displaystyle t}$에 에이전트는 자신의 상태(state) $\displaystyle s_{t}\in S}$와 가능한 행동(action) $\displaystyle A(s_{t})}$를 가지고 있다.

에이전트는 어떤 행동 *a* ∈ *A*(*s**t*) 을 취하고, 환경으로부터 새로운 상태 *s**t*+1와 포상(reward) *r**t*+1을 받는다. 이 상호작용에 기반해서 강화 학습 에이전트는 누적된 포상값 *R*을 최대화하는 정책(policy) π: *S*→*A*을 개발한다.

종료 상태(terminal state)가 존재하는 MDPs에서는 $\displaystyle R=r_{0}+r_{1}+\cdots +r_{n}=\sum _{t=1}^{n}r_{t}}$이고, 그렇지 않은 MDPs에서는 $\displaystyle R=\sum _{t=1}^{n}\gamma ^{t}r_{t}}$가 된다. 여기서 $\displaystyle \gamma }$는 미래의 포상이 현재에 얼마나 가치 있는지를 표현하는 할인율(discount factor)로 0과 1사이의 값이다.

### 응용

강화 학습은 장기, 단기의 포상 사이 트레이드오프가 존재하는 문제를 다루는 데 적합하다. 이것은 로봇 제어, 엘리베이터 스케줄링, 통신망, 백개먼과 체스 같은 게임에 성공적으로 적용되어 왔다.