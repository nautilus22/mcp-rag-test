---
title: 트랜스포머 (기계 학습)
type: 위키피디아 문서
format: markdown
---

# 트랜스포머 (기계 학습)

- 지도 학습
- 비지도 학습
- 온라인 기계 학습
- 메타-학습
- 준지도 학습
- 자기 지도 학습
- 강화 학습
- 규칙 기반 기계 학습
- 뉴로모픽 엔지니어링
- 양자 기계 학습

- 분류
- 생성 모델
- 회귀 분석
- 클러스터 분석
- 차원 축
- 이상 탐지
- 데이터 정제
- AutoML
- 연관 규칙 학습
- 구조 기반 예측
- 특징 공학
- 특징 학습
- 순위 학습
- 문법 유도
- 온톨로지 학습
- 멀티모달 학습

- 결정 트리 학습법
- 앙상블 학습법 배깅 부스팅 랜덤 포레스트
- 최근접 이웃 탐색
- k-NN
- 선형 회귀
- 나이브 베이즈
- 인공신경망
- 로지스틱 회귀
- 퍼셉트론
- 상관 벡터 머신(RVM)
- 서포트 벡터 머신(SVM)

- 배깅
- 부스팅
- 랜덤 포레스트

- BIRCH
- CURE 알고리즘
- 계층적 군집화
- k-평균 알고리즘
- 퍼지 클러스터링
- 기댓값 최대화 알고리즘
- DBSCAN
- OPTICS
- 평균이동

- 인자 분석
- CCA
- 독립 성분 분석
- 선형 판별 분석
- 음수 미포함 행렬 분해
- 주성분 분석
- t-SNE

- 그래프 모형 베이즈 네트워크 조건부 무작위장 은닉 마르코프 모형
- 잠재 디리클레 할당

- 베이즈 네트워크
- 조건부 무작위장
- 은닉 마르코프 모형

- 무작위 표본 합의
- k-최근접 이웃 알고리즘
- 국소 특이점 요인
- 고립 포레스트

- 오토인코더
- 딥 러닝
- 순방향 신경망
- 순환 신경망 LSTM GRU
- 볼츠만 머신 제한된
- 생성적 적대 신경망
- 확산 모델
- 자기조직화 지도
- 합성곱 신경망 U-Net LeNet 알렉스넷 딥드림
- 신경장 신경 방사장 물리정보 신경망
- 트랜스포머 비전
- 맘바
- 스파이킹 신경망
- 멤트렌지스터
- 전기화학 RAM
- 다층 퍼셉트론

- LSTM
- GRU

- 제한된

- U-Net
- LeNet
- 알렉스넷
- 딥드림

- 신경 방사장
- 물리정보 신경망

- 비전

- Q 러닝
- SARSA
- 시간차 학습

- 액티브 러닝
- 크라우드소싱
- 휴먼인더루프
- RLHF

- 결정계수
- 혼동 행렬
- 러닝 커브
- 수신자 조작 특성

- 커널 메소드
- 편향-분산 트레이드오프
- 계산학습이론
- 경험적 위험 최소화
- PAC 러닝
- 통계적 학습이론
- VC 이론

- NeurIPS
- ICML
- ICLR
- ML
- JMLR

- 기계 학습 알고리즘 목록
- 기계 탈학습
- 지식 증류
- 유사도 학습
- 대조 학습

- v
- t
- e

**트랜스포머**(transformer)는 멀티 헤드 [어텐션](https://ko.wikipedia.org/wiki/%EC%96%B4%ED%85%90%EC%85%98_(%EA%B8%B0%EA%B3%84_%ED%95%99%EC%8A%B5)) 메커니즘을 기반으로 하는 [딥 러닝](https://ko.wikipedia.org/wiki/%EB%94%A5_%EB%9F%AC%EB%8B%9D) 아키텍처이며, 텍스트가 [토큰](https://ko.wikipedia.org/wiki/%EB%8C%80%ED%98%95_%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8#토큰화)이라는 수치 표현으로 변환되고 각 토큰은 [워드 임베딩](https://ko.wikipedia.org/wiki/%EC%9B%8C%EB%93%9C_%EC%9E%84%EB%B2%A0%EB%94%A9) 테이블에서 조회를 통해 벡터로 변환된다. 각 레이어에서 각 [토큰](https://ko.wikipedia.org/w/index.php?title=%ED%86%A0%ED%81%B0%ED%99%94_(%EC%96%B4%ED%9C%98_%EB%B6%84%EC%84%9D)&action=edit&redlink=1)은 병렬 멀티 헤드 어텐션 메커니즘을 통해 [컨텍스트 창](https://ko.wikipedia.org/wiki/%EC%BB%A8%ED%85%8D%EC%8A%A4%ED%8A%B8_%EC%B0%BD) 범위 내에서 다른(마스크되지 않은) 토큰과 함께 [컨텍스트화](https://ko.wikipedia.org/w/index.php?title=%EC%BB%A8%ED%85%8D%EC%8A%A4%ED%8A%B8%ED%99%94_(%EC%BB%B4%ED%93%A8%ED%84%B0_%EA%B3%BC%ED%95%99)&action=edit&redlink=1)되며, 이를 통해 핵심 토큰에 대한 신호가 증폭되고 덜 중요한 토큰은 약화된다.

트랜스포머는 순환 유닛이 없다는 장점이 있어 [장단기 메모리](https://ko.wikipedia.org/wiki/%EC%9E%A5%EB%8B%A8%EA%B8%B0_%EB%A9%94%EB%AA%A8%EB%A6%AC) (LSTM)와 같은 이전 [순환 신경망 아키텍처](https://ko.wikipedia.org/wiki/%EC%88%9C%ED%99%98_%EC%8B%A0%EA%B2%BD%EB%A7%9D) (RNN)보다 훈련 시간이 적게 걸린다. 이후의 변형들은 대규모 (언어) [데이터셋](https://ko.wikipedia.org/wiki/%ED%9B%88%EB%A0%A8%EC%9A%A9_%EA%B2%80%EC%A6%9D%EC%9A%A9_%ED%85%8C%EC%8A%A4%ED%8A%B8_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%84%B8%ED%8A%B8)에서 [대형 언어 모델](https://ko.wikipedia.org/wiki/%EB%8C%80%ED%98%95_%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8) (LLM)을 훈련하는 데 널리 채택되었다.

트랜스포머의 현대 버전은 2017년 [구글](https://ko.wikipedia.org/wiki/%EA%B5%AC%EA%B8%80) 연구원들의 논문 "[어텐션 이즈 올 유 니드](https://ko.wikipedia.org/wiki/%EC%96%B4%ED%85%90%EC%85%98_%EC%9D%B4%EC%A6%88_%EC%98%AC_%EC%9C%A0_%EB%8B%88%EB%93%9C)"에서 제안되었다. 트랜스포머는 원래 [기계 번역](https://ko.wikipedia.org/wiki/%EA%B8%B0%EA%B3%84_%EB%B2%88%EC%97%AD)을 위한 이전 아키텍처를 개선하기 위해 개발되었지만, 이후 많은 응용 분야에서 사용되었다. 대규모 [자연어 처리](https://ko.wikipedia.org/wiki/%EC%9E%90%EC%97%B0%EC%96%B4_%EC%B2%98%EB%A6%AC), [컴퓨터 비전](https://ko.wikipedia.org/wiki/%EC%BB%B4%ED%93%A8%ED%84%B0_%EB%B9%84%EC%A0%84) ([비전 트랜스포머](https://ko.wikipedia.org/wiki/%EB%B9%84%EC%A0%84_%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8)), [강화 학습](https://ko.wikipedia.org/wiki/%EA%B0%95%ED%99%94_%ED%95%99%EC%8A%B5), [오디오](https://ko.wikipedia.org/wiki/%EC%98%A4%EB%94%94%EC%98%A4_%EC%8B%A0%ED%98%B8_%EC%B2%98%EB%A6%AC), [멀티모덜 학습](https://ko.wikipedia.org/wiki/%EB%A9%80%ED%8B%B0%EB%AA%A8%EB%8D%9C_%ED%95%99%EC%8A%B5), [로봇공학](https://ko.wikipedia.org/wiki/%EB%A1%9C%EB%B4%87%EA%B3%B5%ED%95%99), 심지어 [컴퓨터 체스](https://ko.wikipedia.org/wiki/%EC%BB%B4%ED%93%A8%ED%84%B0_%EC%B2%B4%EC%8A%A4)를 두는 데에도 사용된다. 또한 [사전 훈련 시스템](https://ko.wikipedia.org/wiki/%EC%A0%84%EC%9D%B4%ED%95%99%EC%8A%B5)인 [GPT](https://ko.wikipedia.org/wiki/GPT_(%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8)) (Generative Pre-trained Transformers) 및 [BERT](https://ko.wikipedia.org/wiki/BERT_(%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8)) (Bidirectional Encoder Representations from Transformers)의 개발로 이어졌다.

### 역사

#### 이전 모델

수년 동안 시퀀스 모델링 및 생성은 일반 [순환 신경망](https://ko.wikipedia.org/wiki/%EC%88%9C%ED%99%98_%EC%8B%A0%EA%B2%BD%EB%A7%9D) (RNN)을 사용하여 수행되었다. 잘 인용된 초기 예는 [엘만 네트워크](https://ko.wikipedia.org/w/index.php?title=%EC%97%98%EB%A7%8C_%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC&action=edit&redlink=1) (1990)였다. 이론적으로 한 토큰의 정보는 시퀀스를 따라 임의로 멀리 전파될 수 있지만, 실제로는 [소실 경사 문제](https://ko.wikipedia.org/w/index.php?title=%EC%86%8C%EC%8B%A4_%EA%B2%BD%EC%82%AC_%EB%AC%B8%EC%A0%9C&action=edit&redlink=1)로 인해 긴 문장 끝에서 모델의 상태가 이전 토큰에 대한 정확하고 추출 가능한 정보 없이 남게 된다.

주요 돌파구는 [LSTM](https://ko.wikipedia.org/wiki/%EC%9E%A5%EB%8B%A8%EA%B8%B0_%EB%A9%94%EB%AA%A8%EB%A6%AC) (1995)이었다. LSTM은 소실 경사 문제를 극복하기 위한 다양한 혁신을 사용하여 긴 시퀀스 모델링을 효율적으로 학습할 수 있었다. 한 가지 주요 혁신은 다른 뉴런의 출력을 곱하는 뉴런, 즉 곱셈 단위를 사용하는 [어텐션 메커니즘](https://ko.wikipedia.org/w/index.php?title=%EC%96%B4%ED%85%90%EC%85%98_%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98&action=edit&redlink=1)을 사용한 것이었다. 곱셈 단위를 사용하는 신경망은 나중에 시그마-파이 네트워크 또는 [고차 네트워크](https://ko.wikipedia.org/w/index.php?title=%EA%B3%A0%EC%B0%A8_%EC%8B%A0%EA%B2%BD%EB%A7%9D&action=edit&redlink=1)라고 불렸다. LSTM은 2017년 트랜스포머 발표 전까지 장기 시퀀스 모델링을 위한 표준 아키텍처가 되었다. 그러나 LSTM은 대부분의 다른 RNN처럼 순차 처리를 사용했다. 특히 RNN은 한 번에 하나의 토큰을 처음부터 끝까지 처리하며, 시퀀스의 모든 토큰에 대해 병렬로 작동할 수 없다.

현대 트랜스포머는 이 문제를 극복했지만, RNN과 달리 컨텍스트 창의 크기에 대해 [이차적](https://ko.wikipedia.org/wiki/%EC%9D%B4%EC%B0%A8_%ED%95%A8%EC%88%98)으로 계산 시간이 필요하다. 선형적으로 스케일링되는 [고속 가중치](https://ko.wikipedia.org/w/index.php?title=%EA%B3%A0%EC%86%8D_%EA%B0%80%EC%A4%91%EC%B9%98&action=edit&redlink=1) 컨트롤러 (1992)는 입력에 따라 추가 처리를 위한 가중치 행렬을 계산하는 방법을 학습한다. 두 네트워크 중 하나는 "고속 가중치" 또는 "동적 링크" (1981)를 가지고 있다. 느린 신경망은 경사 하강법을 통해 쿼리에 대한 답변을 계산하는 빠른 신경망의 가중치 변경을 계산하기 위한 키와 값을 생성하는 방법을 학습한다. 이는 나중에 비정규화된 선형 트랜스포머와 동등하다는 것이 입증되었다.

#### Seq2seq와 어텐션

인코더-디코더 시퀀스 변환 개념은 2010년대 초에 개발되었다. 일반적으로 seq2seq의 원형으로 인용되는 것은 2014년에 동시에 발표된 두 개의 논문이다.

기계 번역을 위한 3억 8천만 매개변수 모델은 두 개의 [장단기 메모리](https://ko.wikipedia.org/wiki/%EC%9E%A5%EB%8B%A8%EA%B8%B0_%EB%A9%94%EB%AA%A8%EB%A6%AC) (LSTM)를 사용한다. 이 아키텍처는 두 부분으로 구성된다. 인코더는 토큰 시퀀스를 입력받아 벡터로 변환하는 LSTM이다. 디코더는 벡터를 토큰 시퀀스로 변환하는 또 다른 LSTM이다. 유사하게, 또 다른 1억 3천만 매개변수 모델은 LSTM 대신 [게이트 순환 유닛](https://ko.wikipedia.org/wiki/%EA%B2%8C%EC%9D%B4%ED%8A%B8_%EC%88%9C%ED%99%98_%EC%9C%A0%EB%8B%9B) (GRU)을 사용했다. 이후 연구에 따르면 GRU는 seq2seq에서 LSTM보다 성능이 더 좋지도 나쁘지도 않다.

이러한 초기 seq2seq 모델에는 어텐션 메커니즘이 없었고, 상태 벡터는 원본 텍스트의 마지막 단어가 처리된 후에만 접근할 수 있었다. 이론적으로 이러한 벡터가 전체 원본 문장에 대한 정보를 유지하지만, 실제로는 정보가 제대로 보존되지 않았다. 이는 입력이 하나의 순환 네트워크에 의해 고정된 크기의 출력 벡터로 순차적으로 처리된 다음, 다른 순환 네트워크에 의해 출력으로 처리되기 때문이다. 입력이 길면 출력 벡터가 모든 관련 정보를 포함할 수 없으므로 출력이 저하된다. 증거로, 입력 문장을 뒤집는 것이 seq2seq 번역을 개선했다.

RNNsearch 모델은 병목 현상 (고정된 크기의 출력 벡터 문제)을 해결하기 위해 기계 번역을 위한 seq2seq에 어텐션 메커니즘을 도입하여 모델이 장거리 의존성을 더 쉽게 처리할 수 있게 했다. 이 이름은 "번역 디코딩 중에 원본 문장을 검색하는 것을 에뮬레이션한다"는 의미이다.

기계 번역을 위한 전역 (RNNsearch의) 및 지역 (슬라이딩 윈도우) 어텐션 모델 아키텍처 간의 상대적 성능을 비교한 결과, 혼합 어텐션이 전역 어텐션보다 품질이 높았고, 지역 어텐션은 번역 시간을 단축했다.

2016년, [구글 번역](https://ko.wikipedia.org/wiki/%EA%B5%AC%EA%B8%80_%EB%B2%88%EC%97%AD)은 [구글 신경망 기계 번역](https://ko.wikipedia.org/wiki/%EA%B5%AC%EA%B8%80_%EC%8B%A0%EA%B2%BD%EB%A7%9D_%EA%B8%B0%EA%B3%84_%EB%B2%88%EC%97%AD)으로 개편되었으며, 이는 이전 [통계적 기계 번역](https://ko.wikipedia.org/wiki/%ED%86%B5%EA%B3%84%EC%A0%81_%EA%B8%B0%EA%B3%84_%EB%B2%88%EC%97%AD) 기반 모델을 대체했다. 새로운 모델은 인코더와 디코더 모두 8계층의 양방향 LSTM으로 구성된 seq2seq 모델이었다. 개발에 9개월이 걸렸으며, 10년이 걸린 통계적 접근 방식을 능가했다.

#### 병렬화 어텐션

어텐션 (자체 어텐션 포함)이 있는 Seq2seq 모델은 여전히 순환 네트워크와 동일한 문제, 즉 [병렬화](https://ko.wikipedia.org/wiki/%EB%B3%91%EB%A0%AC_%EC%BB%B4%ED%93%A8%ED%8C%85)하기 어렵다는 문제로 인해 GPU에서 가속화할 수 없었다. 2016년, 분해 가능한 어텐션은 [순방향 네트워크](https://ko.wikipedia.org/wiki/%EC%88%9C%EB%B0%A9%ED%96%A5_%EC%8B%A0%EA%B2%BD%EB%A7%9D)에 자체 어텐션 메커니즘을 적용하여 쉽게 병렬화할 수 있었고, LSTM보다 매개변수를 10배 적게 사용하여 [텍스트 추론](https://ko.wikipedia.org/w/index.php?title=%ED%85%8D%EC%8A%A4%ED%8A%B8_%EC%B6%94%EB%A1%A0&action=edit&redlink=1)에서 [SOTA](https://ko.wikipedia.org/wiki/%EC%B5%9C%EC%B2%A8%EB%8B%A8) 결과를 달성했다. 저자 중 한 명인 야코프 우슈코레이트(Jakob Uszkoreit)는 순환 없이 어텐션만으로도 언어 번역에 충분할 것이라고 예상했으며, 그래서 "어텐션이 필요한 전부"라는 제목이 붙었다. 당시에는 이 가설이 통념에 반하는 것이었으며, 심지어 그의 아버지이자 유명한 전산 언어학자인 [한스 우슈코레이트](https://ko.wikipedia.org/w/index.php?title=%ED%95%9C%EC%8A%A4_%EC%9A%B0%EC%8A%88%EC%BD%94%EB%A0%88%EC%9D%B4%ED%8A%B8&action=edit&redlink=1)도 회의적이었다. 같은 해, LSTM에 자체 어텐션 (내부 어텐션 또는 문장 내부 어텐션이라 불림)이 제안되었다.

2017년, 원래 (1억 규모의) 인코더-디코더 트랜스포머 모델은 "[어텐션 이즈 올 유 니드](https://ko.wikipedia.org/wiki/%EC%96%B4%ED%85%90%EC%85%98_%EC%9D%B4%EC%A6%88_%EC%98%AC_%EC%9C%A0_%EB%8B%88%EB%93%9C)" 논문에서 제안되었다. 당시 연구의 초점은 [기계 번역](https://ko.wikipedia.org/wiki/%EA%B8%B0%EA%B3%84_%EB%B2%88%EC%97%AD)을 위한 [Seq2seq](https://ko.wikipedia.org/wiki/Seq2seq)를 개선하는 것이었다. 모든 토큰을 병렬로 처리하기 위해 순환을 제거하면서 텍스트 처리 성능을 유지하기 위해 점곱 어텐션 메커니즘을 유지했다. 이는 독립적인 헤드 사용과 순환 부족으로 인해 병렬화하기 더 쉬운 멀티 헤드 어텐션 모델의 도입으로 이어졌다. 이 모델의 병렬화 가능성은 대규모 신경망에서 널리 사용되는 중요한 요인이었다.

#### AI 붐 시대

2017년 봄, "어텐션 이즈 올 유 니드" 초판이 발표되기도 전에 공동 저자 중 한 명은 아키텍처의 "디코더-온리" 변형을 사용하여 가상의 위키백과 기사를 생성했다. 트랜스포머 아키텍처는 현재 진행 중인 [AI 붐](https://ko.wikipedia.org/wiki/AI_%EB%B6%90)에 기여하는 많은 [생성 모델](https://ko.wikipedia.org/wiki/%EC%83%9D%EC%84%B1%ED%98%95_%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5)과 함께 사용된다.

언어 모델링에서 [ELMo](https://ko.wikipedia.org/w/index.php?title=ELMo&action=edit&redlink=1) (2018)는 문맥화된 [워드 임베딩](https://ko.wikipedia.org/wiki/%EC%9B%8C%EB%93%9C_%EC%9E%84%EB%B2%A0%EB%94%A9)을 생성하는 양방향 LSTM으로, [단어 가방 모형](https://ko.wikipedia.org/wiki/%EB%8B%A8%EC%96%B4_%EA%B0%80%EB%B0%A9_%EB%AA%A8%ED%98%95) 및 [Word2vec](https://ko.wikipedia.org/wiki/Word2vec) 연구를 개선했다. 그 뒤를 이어 인코더-온리 트랜스포머 모델인 [BERT](https://ko.wikipedia.org/wiki/BERT_(%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8)) (2018)가 나왔다. 2019년 10월, 구글은 검색 쿼리 처리에 BERT를 사용하기 시작했다. 2020년, 구글 번역은 이전 RNN-인코더-RNN-디코더 모델을 트랜스포머-인코더-RNN-디코더 모델로 교체했다.

2018년부터 오픈AI의 [GPT 시리즈](https://ko.wikipedia.org/wiki/GPT_(%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8))는 디코더-온리 트랜스포머로서 [자연어 생성](https://ko.wikipedia.org/wiki/%EC%9E%90%EC%97%B0%EC%96%B4_%EC%83%9D%EC%84%B1) 분야에서 최첨단이 되었다. 2022년, GPT-3 기반 챗봇인 [챗GPT](https://ko.wikipedia.org/wiki/%EC%B1%97GPT)가 예상치 못하게 인기를 끌면서 [대형 언어 모델](https://ko.wikipedia.org/wiki/%EB%8C%80%ED%98%95_%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8)에 대한 붐을 일으켰다.

2020년부터 트랜스포머는 텍스트를 넘어선 모달리티에 적용되기 시작했으며, 여기에는 [비전 트랜스포머](https://ko.wikipedia.org/wiki/%EB%B9%84%EC%A0%84_%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8), 음성 인식, 로봇공학, 그리고 [멀티모덜](https://ko.wikipedia.org/wiki/%EB%A9%80%ED%8B%B0%EB%AA%A8%EB%8D%9C_%ED%95%99%EC%8A%B5)이 포함된다. 비전 트랜스포머는 다시 [합성곱 신경망](https://ko.wikipedia.org/wiki/%ED%95%A9%EC%84%B1%EA%B3%B1_%EC%8B%A0%EA%B2%BD%EB%A7%9D)의 새로운 발전을 촉진했다. [DALL-E](https://ko.wikipedia.org/wiki/DALL-E) (2021), [스테이블 디퓨전 3](https://ko.wikipedia.org/wiki/%EC%8A%A4%ED%85%8C%EC%9D%B4%EB%B8%94_%EB%94%94%ED%93%A8%EC%A0%84) (2024), 및 [소라](https://ko.wikipedia.org/wiki/%EC%86%8C%EB%9D%BC_(%ED%85%8D%EC%8A%A4%ED%8A%B8-%EB%B9%84%EB%94%94%EC%98%A4_%EB%AA%A8%EB%8D%B8)) (2024)와 같은 이미지 및 비디오 생성기는 트랜스포머를 사용하여 입력 데이터 (텍스트 프롬프트 등)를 "토큰"으로 분해한 다음 자체 어텐션을 사용하여 각 토큰 간의 관련성을 계산하여 모델이 데이터 내의 컨텍스트와 관계를 이해하도록 돕는다.

### 훈련

#### 훈련 안정화 방법

일반 트랜스포머 아키텍처는 수렴에 어려움을 겪었다. 원본 논문에서 저자들은 학습률 웜업을 사용할 것을 권장했다. 즉, 학습률은 훈련의 첫 부분 (보통 총 훈련 단계의 2%로 권장됨) 동안 0에서 최대값으로 선형적으로 스케일링된 후 다시 감소해야 한다.

2020년 논문에서는 멀티헤드 어텐션 및 순방향 레이어 전 (후가 아닌) [레이어 정규화](https://ko.wikipedia.org/w/index.php?title=%EB%A0%88%EC%9D%B4%EC%96%B4_%EC%A0%95%EA%B7%9C%ED%99%94&action=edit&redlink=1)를 사용하면 학습률 웜업 없이 훈련이 안정화된다는 사실을 발견했다.

#### 사전 훈련-미세 조정

트랜스포머는 일반적으로 먼저 대규모 일반 데이터셋에서 [자기 지도 학습](https://ko.wikipedia.org/wiki/%EC%9E%90%EA%B8%B0_%EC%A7%80%EB%8F%84_%ED%95%99%EC%8A%B5)을 통해 사전 훈련된 다음, 작은 작업별 데이터셋에서 [지도](https://ko.wikipedia.org/wiki/%EC%A7%80%EB%8F%84_%ED%95%99%EC%8A%B5) [미세 조정](https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%B8_%ED%8A%9C%EB%8B%9D)된다. 사전 훈련 데이터셋은 일반적으로 [더 파일](https://ko.wikipedia.org/w/index.php?title=%EB%8D%94_%ED%8C%8C%EC%9D%BC_(%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B)&action=edit&redlink=1)과 같은 레이블이 없는 대규모 코퍼스이다. 사전 훈련 및 미세 조정을 위한 작업은 일반적으로 다음을 포함한다.

- 언어 모델링
- 다음 문장 예측
- 질의 응답
- 독해
- 감정 분석
- 다른 말로 표현하기

[T5 트랜스포머](https://ko.wikipedia.org/w/index.php?title=T5_(%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8)&action=edit&redlink=1) 보고서는 수많은 [자연어](https://ko.wikipedia.org/wiki/%EC%9E%90%EC%97%B0%EC%96%B4) 사전 훈련 작업을 문서화한다. 몇 가지 예는 다음과 같다.

- 불완전하거나 손상된 텍스트 복원 또는 수정. 예를 들어, 입력 "Thank you ~~ me to your party ~~ week"는 출력 "Thank you for inviting me to your party last week"를 생성할 수 있다.
- 자연어 간 번역 (기계 번역)
- 자연어의 실용적 허용 여부 판단. 예를 들어, 다음 문장은 문법적으로는 잘 형성되었지만 일반적인 인간의 사용에서는 가능성이 낮기 때문에 "허용되지 않음"으로 판단될 수 있다. The course is jumping well.

이러한 각 작업은 해당 언어의 원어민에게는 사소하거나 명백한 반면, 이전 세대의 기계 학습 아키텍처에서는 일반적으로 어려움을 겪었다는 점에 유의해야 한다.

#### 작업

일반적으로 언어 모델링 작업은 "마스크드", "자기회귀", 및 "prefixLM"의 3가지 클래스로 나뉜다. 이러한 클래스는 트랜스포머와 같은 특정 모델링 아키텍처와는 독립적이지만, 트랜스포머의 맥락에서 종종 논의된다.

마스크드 작업에서, 하나 이상의 토큰이 마스킹되고 모델은 컨텍스트를 기반으로 마스킹된 토큰이 무엇인지 예측하는 확률 분포를 생성한다. 작업의 [손실 함수](https://ko.wikipedia.org/wiki/%EC%86%90%EC%8B%A4_%ED%95%A8%EC%88%98)는 일반적으로 마스킹된 토큰에 대한 [로그-퍼플렉시티](https://ko.wikipedia.org/wiki/%ED%8D%BC%ED%94%8C%EB%A0%89%EC%8B%9C%ED%8B%B0)의 합이다.$\displaystyle {\text{Loss}}=-\sum _{t\in {\text{masked tokens}}}\ln({\text{probability of }}t{\text{ conditional on its context}})}$그리고 모델은 이 손실 함수를 최소화하도록 훈련된다. [BERT 시리즈 모델](https://ko.wikipedia.org/wiki/BERT_(%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8))은 마스크드 토큰 예측 및 다른 작업을 위해 훈련된다.

자기회귀 작업에서, 전체 시퀀스가 처음에는 마스킹되고 모델은 첫 번째 토큰에 대한 확률 분포를 생성한다. 그런 다음 첫 번째 토큰이 드러나고 모델은 두 번째 토큰을 예측하는 식으로 진행된다. 이 작업의 손실 함수는 여전히 일반적으로 동일하다. [GPT 시리즈 모델](https://ko.wikipedia.org/wiki/GPT_(%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8))은 자기회귀 작업을 통해 훈련된다.

prefixLM 작업에서, 시퀀스는 두 부분으로 나뉜다. 첫 번째 부분은 컨텍스트로 제공되며, 모델은 두 번째 부분의 첫 번째 토큰을 예측한다. 그런 다음 해당 토큰이 드러나고, 모델은 두 번째 토큰을 예측하는 식으로 진행된다. 이 작업의 손실 함수는 여전히 일반적으로 동일하다. [T5 시리즈 모델](https://ko.wikipedia.org/w/index.php?title=T5_(%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8)&action=edit&redlink=1)은 prefixLM 작업을 통해 훈련된다.

"마스크드 언어 모델링"의 "마스크드"는 "마스크드 어텐션"의 "마스크드"와 다르며, "prefixLM" (접두사 언어 모델링)은 "prefixLM" (접두사 언어 모델)과 다르다는 점에 유의해야 한다.

### 아키텍처

모든 트랜스포머는 동일한 주요 구성 요소를 가지고 있다.

- 토크나이저: 텍스트를 토큰으로 변환한다.
- 임베딩 레이어: 토큰과 토큰의 위치를 벡터 표현으로 변환한다.
- 트랜스포머 레이어: 벡터 표현에 반복적인 변환을 수행하여 점점 더 많은 언어 정보를 추출한다. 이들은 교대로 어텐션 및 순방향 레이어로 구성된다. 두 가지 주요 유형의 트랜스포머 레이어가 있으며, 더 많은 변형이 있다. 인코더 레이어와 디코더 레이어가 그것이다.
- 언임베딩 레이어: 최종 벡터 표현을 토큰에 대한 확률 분포로 다시 변환한다.

다음 설명은 원본 논문에 기술된 트랜스포머를 정확히 따른다. 다음 섹션에 설명된 변형도 있다.

관례적으로 모든 벡터를 행 벡터로 작성한다. 예를 들어, 벡터를 선형 레이어를 통해 밀어넣는 것은 $\displaystyle xW}$와 같이 오른쪽에 가중치 행렬을 곱하는 것을 의미한다.

#### 토큰화

트랜스포머 아키텍처는 기본적으로 텍스트가 아닌 숫자 데이터를 처리하므로 텍스트와 토큰 간의 변환이 필요하다. 토큰은 문자 또는 짧은 문자 세그먼트를 나타내는 정수이다. 입력 측에서 입력 텍스트는 토큰 시퀀스로 구문 분석된다. 마찬가지로 출력 측에서 출력 토큰은 다시 텍스트로 구문 분석된다. 텍스트와 토큰 시퀀스 간의 변환을 수행하는 모듈은 [토크나이저](https://ko.wikipedia.org/wiki/%EB%82%B1%EB%A7%90_%EB%B6%84%EC%84%9D)이다.

모든 토큰의 집합은 토크나이저의 어휘이며, 그 크기는 *어휘 크기* $\displaystyle n_{\text{vocabulary}}}$이다. 어휘 외의 토큰에 직면했을 때는 일반적으로 "[UNK]"("알 수 없음")라는 특수 토큰이 사용된다.

일반적으로 사용되는 토크나이저로는 바이트 쌍 인코딩, WordPiece, SentencePiece 등이 있다.

#### 임베딩

각 토큰은 [순람표](https://ko.wikipedia.org/wiki/%EC%88%9C%EB%9E%8C%ED%91%9C)를 통해 임베딩 벡터로 변환된다. 동등하게 말하자면, 토큰의 [원-핫](https://ko.wikipedia.org/wiki/%EC%9B%90-%ED%95%AB) 표현에 임베딩 행렬 $\displaystyle M}$을 곱한다. 예를 들어, 입력 토큰이 $\displaystyle 3}$이라면 원-핫 표현은 $\displaystyle [0,0,0,1,0,0,\dots ]}$이고, 그 임베딩 벡터는 다음과 같다. $\displaystyle \mathrm {Embed} (3)=[0,0,0,1,0,0,\dots ]M}$ 토큰 임베딩 벡터는 해당 위치 인코딩 벡터(아래 참조)에 추가되어 입력 벡터 시퀀스를 생성한다.

임베딩 벡터의 차원 수는 *은닉 크기* 또는 *임베딩 크기*라고 불리며 $\displaystyle d_{\text{emb}}}$로 표기된다. 이 크기는 원본 트랜스포머 논문에서 $\displaystyle d_{\text{model}}}$로 표기된다.

#### 언임베딩

언임베딩 레이어는 임베딩 레이어의 거의 역이다. 임베딩 레이어가 토큰을 벡터로 변환하는 반면, 언임베딩 레이어는 벡터를 토큰에 대한 확률 분포로 변환한다.

언임베딩 레이어는 선형-[소프트맥스](https://ko.wikipedia.org/wiki/%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4_%ED%95%A8%EC%88%98) 레이어이다. $\displaystyle \mathrm {UnEmbed} (x)=\mathrm {softmax} (xW+b)}$ 이 행렬의 형태는 $\displaystyle (d_{\text{emb}},n_{\text{vocabulary}})}$이다. 임베딩 행렬 $\displaystyle M}$과 언임베딩 행렬 $\displaystyle W}$는 때때로 서로의 전치 행렬이 되도록 요구되는데, 이를 가중치 연결이라고 한다.

#### 위치 인코딩

위치 인코딩은 시퀀스 내 토큰의 상대적 위치를 고정된 크기의 벡터로 표현한 것이다. 이는 트랜스포머 모델에 입력 시퀀스에서 단어가 *어디에* 있는지에 대한 정보를 제공한다. 이는 입력 시퀀스 "[man bites dog](https://ko.wikipedia.org/w/index.php?title=Man_bites_dog&action=edit&redlink=1)"가 "dog bites man"과 다르게 처리되도록 입력 시퀀스의 순서에 대한 [편향](https://ko.wikipedia.org/wiki/%EA%B7%80%EB%82%A9%EC%A0%81_%ED%8E%B8%ED%96%A5)을 유도한다.

위치 인코딩은 $\displaystyle f:\mathbb {R} \to \mathbb {R} ^{d};d\in \mathbb {Z} ,d>0}$ 유형의 함수로 정의되며, 여기서 $\displaystyle d}$는 양의 짝수 [정수](https://ko.wikipedia.org/wiki/%EC%A0%95%EC%88%98)이다. 원본 논문에 정의된 전체 위치 인코딩은 다음과 같다. $\displaystyle (f(t)_{2k},f(t)_{2k+1})=(\sin(\theta ),\cos(\theta ))\quad \forall k\in \{0,1,\ldots ,d/2-1\}}$ 여기서 $\displaystyle \theta ={\frac {t}{r^{k}}},r=N^{2/d}}$이다.

여기서 $\displaystyle N}$은 위치 인코딩 함수에 입력될 수 있는 가장 큰 $\displaystyle k}$보다 훨씬 커야 하는 자유 매개변수이다. 원본 논문은 $\displaystyle N=10000}$을 사용한다.

이 함수는 $\displaystyle f:\mathbb {R} \to \mathbb {C} ^{d/2}}$ 유형의 복소 함수로 작성될 때 더 간단한 형태를 가진다. $\displaystyle f(t)=\left(e^{it/r^{k}}\right)_{k=0,1,\ldots ,{\frac {d}{2}}-1}}$ 여기서 $\displaystyle r=N^{2/d}}$이다.

이 위치 인코딩 함수를 사용하는 주된 이유는 이를 사용하면 이동이 선형 변환이 되기 때문이다. $\displaystyle f(t+\Delta t)=\mathrm {diag} (f(\Delta t))f(t)}$ 여기서 $\displaystyle \Delta t\in \mathbb {R} }$는 이동하려는 거리이다. 이를 통해 트랜스포머는 인코딩된 모든 위치를 가져와 행렬 곱셈을 통해 n-단계 앞 또는 n-단계 뒤 위치의 인코딩을 찾을 수 있다.

선형 합을 취하면 모든 합성곱도 선형 변환으로 구현될 수 있다. $\displaystyle \sum _{j}c_{j}f(t+\Delta t_{j})=\left(\sum _{j}c_{j}\,\mathrm {diag} (f(\Delta t_{j}))\right)f(t)}$ 어떤 상수 $\displaystyle c_{j}}$에 대해서도 마찬가지이다. 이를 통해 트랜스포머는 인코딩된 모든 위치를 가져와 이웃의 인코딩된 위치의 선형 합을 찾을 수 있다. 인코딩된 위치의 이 합은 어텐션 메커니즘에 입력될 때, [합성곱 신경망](https://ko.wikipedia.org/wiki/%ED%95%A9%EC%84%B1%EA%B3%B1_%EC%8B%A0%EA%B2%BD%EB%A7%9D) [언어 모델](https://ko.wikipedia.org/wiki/%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8)에서 일어나는 것과 유사하게 이웃에 대한 어텐션 가중치를 생성한다. 저자의 말에 따르면, "이것은 모델이 상대적 위치에 따라 쉽게 어텐션하는 방법을 배울 수 있도록 할 것이라고 가정했다."

일반적인 구현에서는 모든 연산이 [복소수](https://ko.wikipedia.org/wiki/%EB%B3%B5%EC%86%8C%EC%88%98)가 아닌 실수에 대해 수행되지만, [복소수 곱셈은 실수 2x2 행렬 곱셈으로 구현될 수 있기](https://ko.wikipedia.org/wiki/%EB%B3%B5%EC%86%8C%EC%88%98#복소수의_행렬_표현) 때문에 이는 단순한 표기법 차이이다.

#### 인코더-디코더 (개요)

이전 [Seq2seq](https://ko.wikipedia.org/wiki/Seq2seq) 모델과 마찬가지로, 원래 트랜스포머 모델은 **인코더-디코더** 아키텍처를 사용했다. 인코더는 모든 입력 토큰을 차례로 처리하는 인코딩 레이어로 구성되는 반면, 디코더는 인코더의 출력과 디코더의 지금까지의 출력 토큰을 반복적으로 처리하는 디코딩 레이어로 구성된다.

각 인코더 레이어의 목적은 토큰의 문맥화된 표현을 생성하는 것이다. 이 표현은 자체 어텐션 메커니즘을 통해 다른 입력 토큰의 정보를 "혼합"하는 토큰에 해당한다. 각 디코더 레이어는 두 개의 어텐션 서브레이어를 포함한다: (1) 인코더의 출력 (문맥화된 입력 토큰 표현)을 통합하기 위한 교차 어텐션, (2) 디코더의 입력 토큰 (즉, 추론 시간 동안 지금까지 생성된 토큰) 사이에서 정보를 "혼합"하기 위한 자체 어텐션.

인코더 및 디코더 레이어 모두 출력의 추가 처리를 위한 [순방향 신경망](https://ko.wikipedia.org/wiki/%EC%88%9C%EB%B0%A9%ED%96%A5_%EC%8B%A0%EA%B2%BD%EB%A7%9D)을 가지며, 잔차 연결 및 레이어 정규화 단계를 포함한다. 이러한 순방향 레이어는 트랜스포머 모델의 매개변수 대부분을 포함한다.

#### 순방향 네트워크

트랜스포머의 순방향 네트워크 (FFN) 모듈은 2계층 [다층 퍼셉트론](https://ko.wikipedia.org/wiki/%EB%8B%A4%EC%B8%B5_%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0)이다. $\displaystyle \mathrm {FFN} (x)=\phi (xW^{(1)}+b^{(1)})W^{(2)}+b^{(2)}}$ 여기서 $\displaystyle W^{(1)}}$ 및 $\displaystyle W^{(2)}}$는 가중치 행렬이고 $\displaystyle b^{(1)}}$ 및 $\displaystyle b^{(2)}}$는 편향 벡터이며, $\displaystyle \phi }$는 [활성화 함수](https://ko.wikipedia.org/wiki/%ED%99%9C%EC%84%B1%ED%99%94_%ED%95%A8%EC%88%98)이다. 원래 트랜스포머는 [ReLU](https://ko.wikipedia.org/wiki/ReLU) 활성화를 사용했다.

중간 계층의 뉴런 수는 *중간 크기* (GPT), *필터 크기* (BERT), 또는 *순방향 크기* (BERT)라고 불린다. 일반적으로 임베딩 크기보다 크다. 예를 들어, GPT-2 시리즈와 BERT 시리즈 모두에서 모델의 중간 크기는 임베딩 크기의 4배이다. $\displaystyle d_{\text{ffn}}=4d_{\text{emb}}}$.

#### 스케일드 점곱 어텐션

##### 어텐션 헤드

트랜스포머 아키텍처에서 사용되는 어텐션 메커니즘은 스케일드 [점곱](https://ko.wikipedia.org/wiki/%EC%8A%A4%EC%B9%BC%EB%9D%BC%EA%B3%B1) [어텐션](https://ko.wikipedia.org/wiki/%EC%96%B4%ED%85%90%EC%85%98_(%EA%B8%B0%EA%B3%84_%ED%95%99%EC%8A%B5)) 유닛이다. 각 유닛에 대해 트랜스포머 모델은 세 가지 가중치 행렬을 학습한다: 쿼리 가중치 $\displaystyle W^{Q}}$, 키 가중치 $\displaystyle W^{K}}$, 값 가중치 $\displaystyle W^{V}}$.

이 모듈은 쿼리 시퀀스, 키 시퀀스, 값 시퀀스 세 가지 시퀀스를 입력으로 받는다. 쿼리 시퀀스는 길이 $\displaystyle \ell _{\text{seq, query}}}$의 시퀀스이며, 각 항목은 차원 $\displaystyle d_{\text{emb, query}}}$의 벡터이다. 키 및 값 시퀀스도 마찬가지이다.

쿼리 시퀀스의 각 벡터 $\displaystyle x_{i,{\text{query}}}}$는 행렬 $\displaystyle W^{Q}}$에 곱해져 쿼리 벡터 $\displaystyle q_{i}=x_{i,{\text{query}}}W^{Q}}$를 생성한다. 모든 쿼리 벡터의 행렬은 쿼리 행렬이다. $\displaystyle Q=X_{\text{query}}W^{Q}}$ 마찬가지로 키 행렬 $\displaystyle K=X_{\text{key}}W^{K}}$와 값 행렬 $\displaystyle V=X_{\text{value}}W^{V}}$를 구성한다.

일반적으로 모든 $\displaystyle W^{Q},W^{K},W^{V}}$는 정방 행렬이며, 이는 $\displaystyle d_{\text{emb, query}}=d_{\text{query}}}$ 등을 의미한다.

어텐션 가중치는 쿼리 및 키 벡터를 사용하여 계산된다. 토큰 $\displaystyle i}$에서 토큰 $\displaystyle j}$까지의 어텐션 가중치 $\displaystyle a_{ij}}$는 $\displaystyle q_{i}}$와 $\displaystyle k_{j}}$ 사이의 [스칼라곱](https://ko.wikipedia.org/wiki/%EC%8A%A4%EC%B9%BC%EB%9D%BC%EA%B3%B1)이다. 어텐션 가중치는 키 벡터의 차원인 $\displaystyle {\sqrt {d_{k}}}}$의 제곱근으로 나뉘어 훈련 중 [기울기](https://ko.wikipedia.org/wiki/%EA%B2%BD%EC%82%AC_%ED%95%98%EA%B0%95%EB%B2%95)를 안정화하고, 가중치를 정규화하는 [소프트맥스](https://ko.wikipedia.org/wiki/%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4_%ED%95%A8%EC%88%98)를 통해 전달된다. $\displaystyle W^{Q}}$와 $\displaystyle W^{K}}$가 다른 행렬이라는 사실은 어텐션이 비대칭일 수 있도록 한다. 토큰 $\displaystyle i}$가 토큰 $\displaystyle j}$에 어텐션한다면 (즉, $\displaystyle q_{i}\cdot k_{j}}$가 크다면), 토큰 $\displaystyle j}$가 토큰 $\displaystyle i}$에 어텐션한다는 것을 반드시 의미하지는 않는다 (즉, $\displaystyle q_{j}\cdot k_{i}}$는 작을 수 있다). 토큰 $\displaystyle i}$에 대한 어텐션 단위의 출력은 모든 토큰의 값 벡터를 $\displaystyle a_{ij}}$ (토큰 $\displaystyle i}$에서 각 토큰으로의 어텐션)로 가중 평균한 값이다.

모든 토큰에 대한 어텐션 계산은 [소프트맥스 함수](https://ko.wikipedia.org/wiki/%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4_%ED%95%A8%EC%88%98)를 사용하여 하나의 큰 행렬 계산으로 표현할 수 있으며, 이는 행렬 연산 최적화를 통해 행렬 연산을 빠르게 계산할 수 있으므로 훈련에 유용하다. 행렬 $\displaystyle Q}$, $\displaystyle K}$, $\displaystyle V}$는 각각 $\displaystyle q_{i}}$, $\displaystyle k_{i}}$, $\displaystyle v_{i}}$ 벡터를 행으로 하는 행렬로 정의된다. 그러면 어텐션을 다음과 같이 표현할 수 있다. $\displaystyle {\begin{aligned}{\text{Attention}}(Q,K,V)={\text{softmax}}\left({\frac {QK^{\mathrm {T} }}{\sqrt {d_{k}}}}\right)V\end{aligned}}}$

여기서 소프트맥스는 행렬의 각 행에 적용된다.

쿼리 벡터의 차원 수는 *쿼리 크기* $\displaystyle d_{\text{query}}}$이고, 마찬가지로 *키 크기* $\displaystyle d_{\text{key}}}$ 및 *값 크기* $\displaystyle d_{\text{value}}}$이다. 어텐션 헤드의 출력 차원은 *헤드 차원* $\displaystyle d_{\text{head}}}$이다. 어텐션 메커니즘은 다음 세 가지 등식이 성립해야 한다. $\displaystyle \ell _{\text{seq, key}}=\ell _{\text{seq, value}},\;d_{\text{query}}=d_{\text{key}},\;d_{\text{value}}=d_{\text{head}}}$ 그렇지 않으면 제약이 없다.

어텐션 헤드가 자체 어텐션 방식으로 사용되면 $\displaystyle X_{\text{query}}=X_{\text{key}}=X_{\text{value}}}$이다. 어텐션 헤드가 교차 어텐션 방식으로 사용되면 일반적으로 $\displaystyle X_{\text{query}}\neq X_{\text{key}}=X_{\text{value}}}$이다. 이론적으로는 세 가지 모두 다를 수 있지만, 실제로는 거의 그렇지 않다.

##### 멀티헤드 어텐션

하나의 $\displaystyle \left(W^{Q},W^{K},W^{V}\right)}$ 행렬 세트를 *어텐션 헤드*라고 하며, 트랜스포머 모델의 각 계층에는 여러 어텐션 헤드가 있다. 각 어텐션 헤드는 각 토큰과 관련된 토큰에 어텐션하는 반면, 여러 어텐션 헤드는 모델이 "관련성"에 대한 다른 정의에 대해 이를 수행할 수 있도록 한다. 특히, 어텐션 점수 계산에 관여하는 쿼리 및 키 투영 행렬인 $\displaystyle W^{Q}}$ 및 $\displaystyle W^{K}}$는 "관련성"을 정의한다. 한편, 값 투영 행렬 $\displaystyle W^{V}}$는 출력 투영 행렬 $\displaystyle W^{O}}$의 부분과 결합하여 어텐션된 토큰이 후속 계층 및 궁극적으로 출력 로짓으로 전달되는 정보에 어떤 영향을 미치는지 결정한다. 또한, 어텐션의 범위, 즉 각 어텐션 헤드가 포착하는 토큰 관계의 범위는 토큰이 연속적인 계층을 통과함에 따라 확장될 수 있다. 이를 통해 모델은 더 깊은 계층에서 더 복잡하고 장거리 의존성을 포착할 수 있다. 많은 트랜스포머 어텐션 헤드는 인간에게 의미 있는 관련성 관계를 인코딩한다. 예를 들어, 일부 어텐션 헤드는 주로 다음 단어에 어텐션하는 반면, 다른 어텐션 헤드는 주로 동사에서 직접 목적어에 어텐션한다. 각 어텐션 헤드에 대한 계산은 [병렬로](https://ko.wikipedia.org/wiki/%EB%B3%91%EB%A0%AC_%EC%BB%B4%ED%93%A8%ED%8C%85) 수행될 수 있어 빠른 처리가 가능하다. 어텐션 레이어의 출력은 [순방향 신경망](https://ko.wikipedia.org/wiki/%EC%88%9C%EB%B0%A9%ED%96%A5_%EC%8B%A0%EA%B2%BD%EB%A7%9D) 레이어로 전달되기 위해 연결된다.

구체적으로, 여러 어텐션 헤드가 $\displaystyle i}$로 인덱싱된다고 하자. 그러면 다음과 같다. $\displaystyle {\text{MultiheadedAttention}}(Q,K,V)={\text{Concat}}_{i\in [n_{\text{heads}}]}({\text{Attention}}(XW_{i}^{Q},XW_{i}^{K},XW_{i}^{V}))W^{O}}$ 여기서 행렬 $\displaystyle X}$는 단어 임베딩의 연결이고, 행렬 $\displaystyle W_{i}^{Q},W_{i}^{K},W_{i}^{V}}$는 개별 어텐션 헤드 $\displaystyle i}$가 소유하는 "투영 행렬"이며, $\displaystyle W^{O}}$는 전체 멀티헤드 어텐션 헤드가 소유하는 최종 투영 행렬이다.

이론적으로 각 어텐션 헤드가 다른 헤드 차원 $\displaystyle d_{\text{head}}}$를 가질 수 있지만, 실제로는 거의 그렇지 않다.

예를 들어, 가장 작은 GPT-2 모델에는 자체 어텐션 메커니즘만 있다. 이 모델은 다음과 같은 차원을 가진다. $\displaystyle d_{\text{emb}}=768,n_{\text{head}}=12,d_{\text{head}}=64}$ $\displaystyle 12\times 64=768}$이므로, 출력 투영 행렬 $\displaystyle W^{O}\in \mathbb {R} ^{(12\times 64)\times 768}}$는 정방 행렬이다.

##### 마스크드 어텐션

트랜스포머 아키텍처는 출력 토큰을 반복적으로 계산하도록 구성된다. $\displaystyle t=0}$이 첫 번째 출력 토큰 $\displaystyle i=0}$의 계산을 나타낸다고 가정하면, 단계 $\displaystyle t>0}$에서 출력 토큰 $\displaystyle i=0}$은 상수로 유지되어야 한다. 이는 모델의 속성이 [자기회귀 모델](https://ko.wikipedia.org/wiki/%EC%9E%90%EA%B8%B0%ED%9A%8C%EA%B7%80%EB%AA%A8%ED%98%95)과 유사하도록 보장한다. 따라서 모든 시간 단계 $\displaystyle t}$에서 모든 출력 $\displaystyle i}$에 대한 계산은 $\displaystyle j>=i}$인 위치 $\displaystyle j}$의 토큰에 접근할 수 없어야 한다 (시간 단계 $\displaystyle t=i}$의 경우 토큰 $\displaystyle j>t}$는 아직 계산되지 않았으므로 당연하다). 이 동작은 소프트맥스 단계 전에 어텐션 링크를 끊어야 하는 항목에는 $\displaystyle -\infty }$를, 다른 항목에는 $\displaystyle 0}$을 갖는 마스크 행렬 $\displaystyle M}$을 추가하여 달성할 수 있다. $\displaystyle {\begin{aligned}{\text{MaskedAttention}}(Q,K,V)={\text{softmax}}\left(M+{\frac {QK^{\mathrm {T} }}{\sqrt {d_{k}}}}\right)V\end{aligned}}}$ 다음 행렬은 디코더 자체 어텐션 모듈에서 일반적으로 사용되며 "인과적 마스킹"이라고 불린다. $\displaystyle M_{\text{causal}}={\begin{bmatrix}0&-\infty &-\infty &\dots &-\infty \\0&0&-\infty &\dots &-\infty \\0&0&0&\dots &-\infty \\\vdots &\vdots &\vdots &\ddots &\vdots \\0&0&0&\dots &0\end{bmatrix}}}$

다시 말해, 각 토큰은 자신과 그 이전의 모든 토큰에 어텐션할 수 있지만, 그 이후의 토큰에는 어텐션할 수 없다. 마스크되지 않은 어텐션 모듈은 마스크의 모든 항목이 0인 마스크드 어텐션 모듈로 생각할 수 있다. 마스크 행렬의 흔치 않은 사용 예시로, [XLNet](https://ko.wikipedia.org/w/index.php?title=XLNet&action=edit&redlink=1)은 $\displaystyle PM_{\text{causal}}P^{-1}}$ 형태의 모든 마스크를 고려하는데, 여기서 $\displaystyle P}$는 무작위 [치환행렬](https://ko.wikipedia.org/wiki/%EC%B9%98%ED%99%98%ED%96%89%EB%A0%AC)이다.

#### 인코더

인코더는 임베딩 레이어와 여러 인코더 레이어로 구성된다.

각 인코더 레이어는 자체 어텐션 메커니즘과 순방향 레이어의 두 가지 주요 구성 요소로 구성된다. 이 레이어는 입력 벡터 시퀀스를 입력으로 받아 자체 어텐션 메커니즘을 적용하여 중간 벡터 시퀀스를 생성한 다음, 각 벡터에 개별적으로 순방향 레이어를 적용한다. 개략적으로 다음과 같다. $\displaystyle {\begin{aligned}{\text{given input vectors }}&h_{0},h_{1},\dots \\{\text{combine them into a matrix }}H&={\begin{bmatrix}h_{0}\\h_{1}\\\vdots \end{bmatrix}}\\{\text{EncoderLayer}}(H)&={\begin{bmatrix}{\text{FFN}}({\text{MultiheadedAttention}}(H,H,H)_{0})\\{\text{FFN}}({\text{MultiheadedAttention}}(H,H,H)_{1})\\\vdots \end{bmatrix}}\\\end{aligned}}}$

여기서 $\displaystyle {\text{FFN}}}$은 "순방향 네트워크"를 나타낸다. 이를 더 간결하게 다음과 같이 쓸 수 있다. $\displaystyle {\text{EncoderLayer}}(H)={\text{FFN}}({\text{MultiheadedAttention}}(H,H,H))}$ $\displaystyle {\text{FFN}}}$은 행렬의 각 행에 개별적으로 적용된다는 암묵적인 관례가 있다.

인코더 레이어는 쌓여 있다. 첫 번째 인코더 레이어는 임베딩 레이어에서 입력 벡터 시퀀스를 받아 벡터 시퀀스를 생성한다. 이 벡터 시퀀스는 두 번째 인코더에 의해 처리되고, 이런 식으로 계속된다. 최종 인코더 레이어의 출력은 디코더에 의해 사용된다.

인코더가 전체 입력을 한 번에 처리하므로 모든 토큰은 다른 모든 토큰에 어텐션할 수 있으므로(모든 대 모든 어텐션) 인과적 마스킹이 필요 없다.

#### 디코더

디코더는 임베딩 레이어, 여러 디코더 레이어, 그리고 언임베딩 레이어로 구성된다.

각 디코더는 세 가지 주요 구성 요소로 구성된다: 인과적으로 마스킹된 자체 어텐션 메커니즘, 교차 어텐션 메커니즘, 그리고 순방향 신경망이다. 디코더는 인코더와 유사하게 작동하지만, 인코더가 생성한 인코딩에서 관련 정보를 추출하는 추가적인 어텐션 메커니즘이 삽입된다. 이 메커니즘은 *인코더-디코더 어텐션*이라고도 불린다.

첫 번째 인코더와 마찬가지로, 첫 번째 디코더는 인코딩 대신 출력 시퀀스의 위치 정보와 임베딩을 입력으로 받는다. 트랜스포머는 출력을 예측하기 위해 현재 또는 미래의 출력을 사용해서는 안 되므로, 이러한 역방향 정보 흐름을 방지하기 위해 출력 시퀀스를 부분적으로 마스킹해야 한다. 이를 통해 [자기회귀](https://ko.wikipedia.org/wiki/%EC%9E%90%EA%B8%B0%ED%9A%8C%EA%B7%80%EB%AA%A8%ED%98%95) 텍스트 생성이 가능하다. 디코딩의 경우, 토큰이 아직 생성되지 않았으므로 토큰이 아직 생성되지 않은 토큰에 어텐션할 수 없기 때문에 모든 대 모든 어텐션은 부적절하다. 따라서 디코더의 자체 어텐션 모듈은 인과적으로 마스킹된다.

대조적으로, 교차 어텐션 메커니즘은 디코더가 디코딩을 시작하기 전에 계산되는 인코더의 출력 벡터에 어텐션한다. 결과적으로 교차 어텐션 메커니즘에서는 마스킹이 필요 없다.

개략적으로 다음과 같다. $\displaystyle {\begin{aligned}H'&={\text{MaskedMultiheadedAttention}}(H,H,H)\\{\text{DecoderLayer}}(H)&={\text{FFN}}({\text{MultiheadedAttention}}(H',H^{E},H^{E}))\end{aligned}}}$ 여기서 $\displaystyle H^{E}}$는 행이 인코더의 출력 벡터인 행렬이다.

마지막 디코더 뒤에는 최종 언임베딩 레이어가 이어져 어휘에 대한 출력 확률을 생성한다. 그런 다음 확률에 따라 토큰 중 하나가 샘플링되고, 디코더를 다시 실행하여 다음 토큰을 생성하는 방식으로 자기회귀적으로 출력 텍스트를 생성할 수 있다.

#### 수정된 아키텍처

많은 [대형 언어 모델](https://ko.wikipedia.org/wiki/%EB%8C%80%ED%98%95_%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8)은 입력 시퀀스로부터 완전히 새로운 시퀀스를 예측할 필요가 없으므로, 원래 트랜스포머 아키텍처의 인코더 또는 디코더만 사용한다. 초기 [GPT](https://ko.wikipedia.org/wiki/GPT_(%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8)) 모델은 시퀀스의 다음 토큰을 예측하도록 훈련된 디코더-온리 모델이다. 다른 언어 모델인 [BERT](https://ko.wikipedia.org/wiki/BERT_(%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8))는 인코더만 사용하며, 시퀀스에서 무작위로 마스킹된 토큰을 예측하도록 훈련된다.

### 전체 트랜스포머 아키텍처

#### 서브레이어

각 인코더 레이어는 자체 어텐션과 순방향 네트워크의 2개의 서브레이어를 포함한다. 각 디코더 레이어는 인과적으로 마스킹된 자체 어텐션, 교차 어텐션, 순방향 네트워크의 3개의 서브레이어를 포함한다.

마지막 세부 사항은 [잔차 연결](https://ko.wikipedia.org/wiki/%EC%9E%94%EC%B0%A8_%EC%8B%A0%EA%B2%BD%EB%A7%9D)과 [레이어 정규화](https://ko.wikipedia.org/w/index.php?title=%EB%A0%88%EC%9D%B4%EC%96%B4_%EC%A0%95%EA%B7%9C%ED%99%94&action=edit&redlink=1) (LayerNorm, 또는 LN)인데, 개념적으로는 불필요하지만 수치적 안정성과 수렴을 위해 필요하다.

소실 경사 문제를 피하고 훈련 과정을 안정화하기 위해 도입된 잔차 연결은 y = F(x) + x로 표현될 수 있다. 이 표현은 출력 y가 입력 x의 변환(F(x))과 입력 자체(x)의 합임을 나타낸다. 입력 x를 추가하면 입력 정보를 보존하고 F(x)의 경사가 거의 0에 가까울 때 발생하는 문제를 피할 수 있다.

순방향 네트워크 모듈이 각 벡터에 개별적으로 적용되는 것과 유사하게, 레이어 정규화(LayerNorm)도 각 벡터에 개별적으로 적용된다.

두 가지 일반적인 컨벤션이 사용된다: *Post-LN*과 *Pre-LN* 컨벤션. Post-LN 컨벤션에서 각 서브레이어의 출력은 다음과 같다. $\displaystyle \mathrm {LayerNorm} (x+\mathrm {Sublayer} (x))}$ 여기서 $\displaystyle \mathrm {Sublayer} (x)}$는 서브레이어 자체에서 구현된 함수이다.

Pre-LN 컨벤션에서 각 서브레이어의 출력은 다음과 같다. $\displaystyle x+\mathrm {Sublayer} (\mathrm {LayerNorm} (x))}$ 원래 2017년 트랜스포머는 Post-LN 컨벤션을 사용했다. 이는 훈련하기 어려웠고 신중한 [하이퍼파라미터](https://ko.wikipedia.org/wiki/%ED%95%98%EC%9D%B4%ED%8D%BC%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0_(%EA%B8%B0%EA%B3%84_%ED%95%99%EC%8A%B5)) 튜닝과 학습률의 "웜업"이 필요했는데, 이는 학습률이 작게 시작하여 점진적으로 증가하는 방식이었다. 2018년에 여러 차례 제안된 Pre-LN 컨벤션은 웜업이 필요 없이 훈련하기 더 쉽고 더 빠른 수렴을 이끌어낸다는 것이 밝혀졌다.

#### 의사 코드

다음은 표준 Pre-LN 인코더-디코더 트랜스포머의 의사 코드이며에서 발췌했다.

#### 용어

트랜스포머 아키텍처는 모듈화되어 다양한 변형을 허용한다. 몇 가지 일반적인 변형이 여기에 설명되어 있다.

"인코더-온리" 트랜스포머는 인코더를 적용하여 입력 텍스트를 입력 텍스트를 나타내는 벡터 시퀀스로 매핑한다. 이는 일반적으로 텍스트 임베딩 및 후속 애플리케이션을 위한 [표현 학습](https://ko.wikipedia.org/wiki/%ED%8A%B9%EC%A7%95_%ED%95%99%EC%8A%B5)에 사용된다. [BERT](https://ko.wikipedia.org/wiki/BERT_(%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8))는 인코더-온리이다. 인코더-디코더 트랜스포머를 훈련한 다음 인코더만 사용하는 것보다 훨씬 좋지 않다는 것이 밝혀졌으므로 현재는 덜 사용된다.

"디코더-온리" 트랜스포머는 문자 그대로 디코더-온리가 아니다. 인코더가 없으면 교차 어텐션 메커니즘은 어텐션할 대상이 없다. 따라서 디코더-온리 트랜스포머의 디코더 레이어는 인과적으로 마스킹된 자체 어텐션과 순방향 네트워크라는 두 개의 서브레이어로만 구성된다. 이는 일반적으로 [텍스트 생성](https://ko.wikipedia.org/wiki/%EC%9E%90%EC%97%B0%EC%96%B4_%EC%83%9D%EC%84%B1) 및 [명령 따르기](https://ko.wikipedia.org/wiki/%EB%8C%80%ED%98%95_%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8#명령_튜닝)에 사용된다. [GPT 시리즈](https://ko.wikipedia.org/wiki/GPT_(%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8)) 및 [친칠라 시리즈](https://ko.wikipedia.org/w/index.php?title=%EC%B9%9C%EC%B9%A0%EB%9D%BC_(%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8)&action=edit&redlink=1)의 모델은 디코더-온리이다.

"인코더-디코더" 트랜스포머는 일반적으로 원본 트랜스포머와 동일하며, 각 인코더 레이어당 2개의 서브레이어와 각 디코더 레이어당 3개의 서브레이어 등이 있다. 대체 활성화 함수, 정규화 위치 변경 등과 같은 사소한 아키텍처 개선 사항이 있을 수 있다. 이는 일반적으로 텍스트 생성 및 명령 따르기에도 사용된다. [T5 시리즈](https://ko.wikipedia.org/w/index.php?title=T5_(%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8)&action=edit&redlink=1)의 모델은 인코더-디코더이다.

"prefixLM" (접두사 언어 모델)은 디코더 전용 아키텍처이지만, 인과적 마스킹과 다른 접두사 마스킹을 사용한다.$\displaystyle M_{\text{prefixLM}}={\begin{bmatrix}\mathbf {0} &-\infty \\\mathbf {0} &M_{\text{causal}}\end{bmatrix}}}$ 여기서 첫 번째 열은 "접두사"에 해당하고, 이어지는 열은 접두사를 기반으로 자기회귀적으로 생성된 텍스트에 해당한다. 이는 인코더-디코더 모델과 유사하지만 "희소성"이 적다. 이러한 모델은 이론적 가능성 및 벤치마크 비교로 인용되기는 하지만 거의 사용되지 않는다.

혼합형 seq2seq 모델도 있다. 예를 들어, 2020년 구글 번역은 RNN-디코더가 자기회귀적으로 실행될 때 트랜스포머-디코더보다 훨씬 빠르게 실행된다는 주장에 따라 이전 RNN-인코더-RNN-디코더 모델을 트랜스포머-인코더-RNN-디코더 모델로 교체했다.

### 후속 연구

#### 대체 활성화 함수

원래 트랜스포머는 [ReLU](https://ko.wikipedia.org/wiki/ReLU) [활성화 함수](https://ko.wikipedia.org/wiki/%ED%99%9C%EC%84%B1%ED%99%94_%ED%95%A8%EC%88%98)를 사용한다. 다른 활성화 함수들이 개발되었다. [라마 시리즈](https://ko.wikipedia.org/wiki/LLaMA)와 [PaLM](https://ko.wikipedia.org/wiki/PaLM)은 SwiGLU를 사용했고; GPT-1과 BERT 모두 GELU를 사용했다.

대체 활성화 함수는 순방향 모듈에서 [게이트 선형 단위](https://ko.wikipedia.org/w/index.php?title=%EA%B2%8C%EC%9D%B4%ED%8A%B8_%EC%84%A0%ED%98%95_%EB%8B%A8%EC%9C%84&action=edit&redlink=1)와 함께 사용되는 경우가 많다.

#### 대체 정규화

트랜스포머에 사용되는 정규화는 레이어 정규화(LayerNorm)와 다를 수 있다. 한 가지 예는 [라마 시리즈](https://ko.wikipedia.org/wiki/LLaMA)에서 사용되는 [RMSNorm](https://ko.wikipedia.org/w/index.php?title=RMSNorm&action=edit&redlink=1)이다. 다른 예로는 CapsuleNorm ScaleNorm, 또는 FixNorm.

#### 대체 위치 인코딩

트랜스포머는 사인파형 외에 다른 위치 인코딩 방법을 사용할 수 있다.

원래 트랜스포머 논문에서는 학습된 위치 인코딩을 사용했다고 보고했지만, 사인파형 인코딩보다 우수하지 않다는 것을 발견했다. 이후,는 인과적 마스킹 자체가 트랜스포머 디코더에 충분한 신호를 제공하여 위치 인코딩 모듈 없이도 절대 위치 인코딩을 암시적으로 수행할 수 있음을 발견했다.

##### RoPE

RoPE (회전 위치 임베딩)는 2차원 벡터 목록 $\displaystyle [(x_{1}^{(1)},x_{1}^{(2)}),(x_{2}^{(1)},x_{2}^{(2)}),(x_{3}^{(1)},x_{3}^{(2)}),...]}$를 고려하면 가장 잘 설명된다. 이제 어떤 각도 $\displaystyle \theta }$를 선택한다. 그러면 RoPE 인코딩은 다음과 같다. $\displaystyle {\text{RoPE}}{\big (}x_{m}^{(1)},x_{m}^{(2)},m{\big )}={\begin{pmatrix}\cos m\theta &-\sin m\theta \\\sin m\theta &\cos m\theta \end{pmatrix}}{\begin{pmatrix}x_{m}^{(1)}\\x_{m}^{(2)}\\\end{pmatrix}}={\begin{pmatrix}x_{m}^{(1)}\cos m\theta -x_{m}^{(2)}\sin m\theta \\x_{m}^{(2)}\cos m\theta +x_{m}^{(1)}\sin m\theta \\\end{pmatrix}}}$ 동등하게, 2차원 벡터를 복소수 $\displaystyle z_{m}:=x_{m}^{(1)}+ix_{m}^{(2)}}$로 작성하면, RoPE 인코딩은 단순히 각도 곱셈이다. $\displaystyle {\text{RoPE}}{\big (}z_{m},m{\big )}=e^{im\theta }z_{m}}$ $\displaystyle 2n}$차원 벡터 목록의 경우, RoPE 인코더는 각도 시퀀스 $\displaystyle \theta ^{(1)},...,\theta ^{(n)}}$에 의해 정의된다. 그러면 RoPE 인코딩은 각 좌표 쌍에 적용된다.

RoPE의 이점은 두 벡터 간의 점곱이 상대적 위치에만 의존한다는 것이다. $\displaystyle {\text{RoPE}}{\big (}x,m{\big )}^{T}{\text{RoPE}}{\big (}y,n{\big )}={\text{RoPE}}{\big (}x,m+k{\big )}^{T}{\text{RoPE}}{\big (}y,n+k{\big )}}$ 어떤 정수 $\displaystyle k}$에 대해서도 마찬가지이다.

##### ALiBi

ALiBi (Attention with Linear Biases)는 원래 트랜스포머의 위치 인코더를 *대체하는* 것이 아니다. 대신, 어텐션 메커니즘에 직접 연결되는 *추가적인* 위치 인코더이다. 구체적으로, ALiBi 어텐션 메커니즘은 다음과 같다. $\displaystyle {\begin{aligned}{\text{Attention}}(Q,K,V)={\text{softmax}}\left({\frac {QK^{\mathrm {T} }}{\sqrt {d_{k}}}}+sB\right)V\end{aligned}}}$ 여기서 $\displaystyle s}$는 실수("스칼라")이고, $\displaystyle B}$는 다음과 같이 정의되는 *선형 편향* 행렬이다. $\displaystyle B={\begin{pmatrix}0&1&2&3&\cdots \\-1&0&1&2&\cdots \\-2&-1&0&1&\cdots \\-3&-2&-1&0&\cdots \\\vdots &\vdots &\vdots &\vdots &\ddots \\\end{pmatrix}}}$ 다른 말로, $\displaystyle B_{i,j}=j-i}$이다. 선형 편향 행렬은 부드러운 마스크라는 아이디어이다. $\displaystyle 0}$은 완전한 어텐션을 나타내고, $\displaystyle -\infty }$는 어텐션을 나타내지 않는 것처럼, 선형 편향 행렬은 한 방향으로는 어텐션을 증가시키고 다른 방향으로는 어텐션을 감소시킨다.

ALiBi는 짧은 컨텍스트 창에서 사전 훈련한 다음, 더 긴 컨텍스트 창에서 미세 조정하는 것을 가능하게 한다. 어텐션 메커니즘에 직접 연결되기 때문에 전체 네트워크의 "바닥"에 연결되는 모든 위치 인코더 (원본 트랜스포머의 사인파 인코더, RoPE 및 기타 여러 가지)와 결합할 수 있다.

##### 상대 위치 인코딩

상대 위치 인코딩은 ALiBi와 유사하지만 더 일반적이다. $\displaystyle {\begin{aligned}{\text{Attention}}(Q,K,V)={\text{softmax}}\left({\frac {QK^{\mathrm {T} }}{\sqrt {d_{k}}}}+B\right)V\end{aligned}}}$ 여기서 $\displaystyle B}$는 [퇴플리츠 행렬](https://ko.wikipedia.org/wiki/%ED%87%B4%ED%94%8C%EB%A6%AC%EC%B8%A0_%ED%96%89%EB%A0%AC)이며, 즉 $\displaystyle i-j=i'-j'}$일 때마다 $\displaystyle B_{i,j}=B_{i',j'}}$이다. 이는 "절대 위치 인코딩"인 원래의 사인파 위치 인코딩과 대조된다.

#### 효율적인 구현

트랜스포머 모델은 [텐서플로](https://ko.wikipedia.org/wiki/%ED%85%90%EC%84%9C%ED%94%8C%EB%A1%9C) 및 [PyTorch](https://ko.wikipedia.org/wiki/PyTorch)와 같은 표준 [딥 러닝 프레임워크](https://ko.wikipedia.org/w/index.php?title=%ED%94%84%EB%A0%88%EC%9E%84%EC%9B%8C%ED%81%AC_(%EC%BB%B4%ED%93%A8%ED%84%B0_%EA%B3%BC%ED%95%99)&action=edit&redlink=1)에 구현되었다. *트랜스포머*는 [허깅 페이스](https://ko.wikipedia.org/wiki/%ED%97%88%EA%B9%85_%ED%8E%98%EC%9D%B4%EC%8A%A4)가 개발한 라이브러리로, 트랜스포머 기반 아키텍처와 사전 훈련된 모델을 제공한다.

##### KV 캐싱

자기회귀 트랜스포머가 텍스트 생성과 같은 추론에 사용될 때, 쿼리 벡터는 각 단계에서 다르지만, 이미 계산된 키 및 값 벡터는 항상 동일하다. **KV 캐싱** 방법은 각 어텐션 블록에서 계산된 키 및 값 벡터를 저장하여 각 새 토큰에서 다시 계산되지 않도록 한다. **PagedAttention**은 KV 캐싱에 [메모리 페이징](https://ko.wikipedia.org/wiki/%ED%8E%98%EC%9D%B4%EC%A7%95)을 적용한다.

["당신은 고객 지원 상담원입니다..."]와 같이 내장된 프롬프트와 함께 트랜스포머가 사용되는 경우, 프롬프트에 대한 키 및 값 벡터를 계산하고 디스크에 저장할 수 있다. 모델이 온라인 챗봇과 같이 많은 짧은 상호 작용에 사용될 때 계산 절약은 상당하다.

##### FlashAttention

FlashAttention는 GPU에서 트랜스포머 어텐션 메커니즘을 효율적으로 구현하는 알고리즘이다. 이는 [블록 단위로 행렬 곱셈](https://ko.wikipedia.org/wiki/%EB%B8%94%EB%A1%9D_%ED%96%89%EB%A0%AC#블록_행렬_연산)을 수행하여 각 블록이 GPU의 [캐시](https://ko.wikipedia.org/wiki/%EC%BA%90%EC%8B%9C) 내에 맞도록 하고, 블록을 신중하게 관리하여 GPU 캐시 간의 데이터 복사를 최소화하는 (데이터 이동이 느리기 때문에) 통신 회피 알고리즘이다. 자세한 내용은 [소프트맥스](https://ko.wikipedia.org/wiki/%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4_%ED%95%A8%EC%88%98#수치_알고리즘) 페이지를 참조한다.

개선된 버전인 FlashAttention-2는 더 긴 컨텍스트 길이를 처리할 수 있는 언어 모델에 대한 증가하는 수요를 충족시키기 위해 개발되었다. 작업 분할 및 병렬 처리 개선을 제공하여 [A100](https://ko.wikipedia.org/wiki/Nvidia_A100) GPU ([FP16](https://ko.wikipedia.org/wiki/FP16)/[BF16](https://ko.wikipedia.org/w/index.php?title=BF16&action=edit&redlink=1))에서 최대 230 TFLOPs/s를 달성하여 원래 FlashAttention보다 2배 빠른 속도를 제공한다.

FlashAttention-2의 주요 발전 사항은 비-행렬 곱셈 FLOPS 감소, 시퀀스 길이 차원에 대한 병렬 처리 개선, GPU 워프 간의 더 나은 작업 분할, 그리고 최대 256의 헤드 차원과 멀티 쿼리 어텐션(MQA) 및 그룹 쿼리 어텐션(GQA)에 대한 추가 지원을 포함한다.

벤치마크 결과 FlashAttention-2는 FlashAttention보다 최대 2배 빠르고 PyTorch의 표준 어텐션 구현보다 최대 9배 빠르다. 향후 개발에는 [H100](https://ko.wikipedia.org/w/index.php?title=Nvidia_H100&action=edit&redlink=1) GPU와 FP8과 같은 새로운 데이터 유형과 같은 새로운 하드웨어에 대한 최적화가 포함된다.

##### 멀티 쿼리 어텐션

멀티 쿼리 어텐션은 멀티헤드 어텐션 메커니즘을 변경한다. 보통은 다음과 같지만,

$\displaystyle {\text{MultiheadedAttention}}(Q,K,V)={\text{Concat}}_{i\in [n_{\text{heads}}]}\left({\text{Attention}}(XW_{i}^{Q},XW_{i}^{K},XW_{i}^{V})\right)W^{O}}$ 멀티 쿼리 어텐션에서는 $\displaystyle W^{K},W^{V}}$가 하나만 있으므로 다음과 같다.

$\displaystyle {\text{MultiQueryAttention}}(Q,K,V)={\text{Concat}}_{i\in [n_{\text{heads}}]}\left({\text{Attention}}(XW_{i}^{Q},XW^{K},XW^{V})\right)W^{O}}$

이는 모델 품질과 훈련 속도에 중립적인 영향을 미치지만, 추론 속도를 향상시킨다.

더 일반적으로, 그룹 쿼리 어텐션(GQA)은 어텐션 헤드를 그룹으로 나누며, 각 그룹은 키-값 쌍을 공유한다. MQA는 그룹이 하나인 GQA이고, 표준 멀티헤드 어텐션은 그룹 수가 최대인 GQA이다.

멀티헤드 잠재 어텐션(MLA)은 표준 MHA의 [저계수 근사](https://ko.wikipedia.org/w/index.php?title=%EC%A0%80%EA%B3%84%EC%88%98_%EA%B7%BC%EC%82%AC&action=edit&redlink=1)이다. 구체적으로, 각 은닉 벡터는 어텐션 메커니즘에 들어가기 전에 먼저 두 개의 저차원 공간("잠재 공간")으로 투영되는데, 하나는 쿼리용이고 다른 하나는 키-값(KV 벡터)용이다. 이 설계는 KV 캐시를 최소화한다. 왜냐하면 저차원 KV 벡터만 캐시하면 되기 때문이다.

##### 추론적 디코딩

추론적 디코딩은 토큰 디코딩 속도를 높이는 방법이다. [CPU의 투기적 실행](https://ko.wikipedia.org/wiki/%ED%88%AC%EA%B8%B0%EC%A0%81_%EC%8B%A4%ED%96%89)과 유사하게, 미래 토큰은 빠르게 계산된 다음 검증된다. 빠르게 계산된 토큰이 부정확하면 폐기되고 느리게 다시 계산된다.

추론적 디코딩의 핵심은 트랜스포머 디코더가 다음과 같은 의미에서 디코딩하는 것보다 더 빠르게 검증할 수 있다는 점이다.

GPT-3 및 GPT-3-small과 같이 컨텍스트 창 크기가 512인 두 개의 트랜스포머 모델이 있다고 가정하자. GPT-3로 전체 컨텍스트 창을 탐욕적 디코딩으로 자기회귀적으로 생성하려면, 각 토큰 $\displaystyle x_{1},x_{2},...,x_{512}}$를 생성할 때마다 512번 실행되어야 하며, 총 시간은 $\displaystyle 512T_{\text{GPT-3}}}$가 걸린다. 그러나 이러한 토큰 값에 대해 교육받은 추측이 있다면, 모델을 한 번 실행하여 모든 토큰을 병렬로 검증할 수 있다. 각 $\displaystyle x_{t}}$가 실제로 $\displaystyle t}$번째 출력에서 가장 큰 로그 우도(log-likelihood)를 가진 토큰인지 확인하는 것이다.

추론적 디코딩에서는 더 작은 모델이나 다른 간단한 휴리스틱을 사용하여 몇 개의 추론적 토큰을 생성한 다음 더 큰 모델에 의해 검증된다. 예를 들어, GPT-3-small을 사용하여 네 개의 추론적 토큰 $\displaystyle {\tilde {x}}_{1},{\tilde {x}}_{2},{\tilde {x}}_{3},{\tilde {x}}_{4}}$를 생성한다고 가정하자. 이는 $\displaystyle 4T_{\text{GPT-3-small}}}$만 소요된다. 이 토큰들은 더 큰 GPT-3를 통해 한 번에 실행된다. $\displaystyle {\tilde {x}}_{1}}$과 $\displaystyle {\tilde {x}}_{2}}$가 GPT-3에 의해 선택될 것이라고 확인되면 이들은 유지되지만, $\displaystyle {\tilde {x}}_{3}}$은 그렇지 않으므로 $\displaystyle {\tilde {x}}_{3},{\tilde {x}}_{4}}$는 폐기되고, GPT-3는 이들에 대해 실행된다. 이 경우 $\displaystyle 4T_{\text{GPT-3-small}}+3T_{\text{GPT-3}}}$가 소요되며, 이는 $\displaystyle 4T_{\text{GPT-3}}}$보다 짧을 수 있다.

비탐욕적 디코딩의 경우, 유사한 아이디어가 적용되지만, 추론적 토큰은 확률적으로 수용되거나 거부되며, 이는 최종 출력 분포가 추론적 디코딩을 사용하지 않은 경우와 동일하도록 보장하는 방식으로 이루어진다.

멀티 토큰 예측에서 단일 순방향 패스는 최종 임베딩 벡터를 생성하며, 이 벡터는 다시 토큰 확률로 언임베딩된다. 그러나 이 벡터는 다른 트랜스포머 블록에 의해 추가로 처리되어 다음 토큰을 예측할 수 있으며, 미래의 임의의 많은 단계에 대해 이러한 방식으로 계속될 수 있다. 이는 각 새 토큰이 전체 스택이 아닌 단 하나의 트랜스포머 블록만 비용을 지불하므로 정확도를 속도와 교환하는 것이다.

#### 이차 미만 트랜스포머

트랜스포머 기반 아키텍처 훈련은 특히 긴 입력의 경우 비용이 많이 들 수 있다. 이 문제를 해결하기 위해 많은 방법이 개발되었다. 이미지 도메인에서 Swin Transformer는 이동 창 내에서 어텐션을 수행하는 효율적인 아키텍처이다. 오디오 도메인에서 SepTr은 시간 및 주파수 도메인에서 어텐션을 분리한다. *Long Range Arena* (2020)는 긴 입력에 대한 트랜스포머 아키텍처의 동작을 비교하는 표준 벤치마크이다.

##### 대체 어텐션 그래프

표준 어텐션 그래프는 모든 대 모든 또는 인과적이며, 둘 다 $\displaystyle N}$이 시퀀스의 토큰 수일 때 $\displaystyle O(N^{2})}$로 스케일링된다.

리포머 (2020)는 [지역 민감 해싱](https://ko.wikipedia.org/w/index.php?title=%EC%A7%80%EC%97%AD_%EB%AF%BC%EA%B0%90_%ED%95%B4%EC%8B%B1&action=edit&redlink=1)과 가역 계층을 사용하여 계산 부하를 $\displaystyle O(N^{2})}$에서 $\displaystyle O(N\ln N)}$으로 줄인다.

희소 어텐션은 $\displaystyle O(N^{2})}$보다 느리게 증가하는 어텐션 그래프를 사용한다. 예를 들어, BigBird (2020)는 $\displaystyle O(N)}$로 증가하는 무작위 [작은 세상 네트워크](https://ko.wikipedia.org/wiki/%EC%9E%91%EC%9D%80_%EC%84%B8%EC%83%81_%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC)를 사용한다.

일반 트랜스포머는 컨텍스트 창 크기에 대해 이차적인 메모리 크기를 필요로 한다. 어텐션이 없는 트랜스포머는 키를 값에 연결하여 이를 선형 종속성으로 줄이면서도 트랜스포머의 장점을 유지한다.

##### 랜덤 특징 어텐션

랜덤 특징 어텐션(Random Feature Attention) (2021)은 [푸리에 랜덤 특징](https://ko.wikipedia.org/w/index.php?title=%EB%B0%A9%EC%82%AC%ED%98%95_%EA%B8%B0%EC%A0%80_%ED%95%A8%EC%88%98_%ED%95%B5&action=edit&redlink=1)을 사용한다. $\displaystyle \varphi (x)={\frac {1}{\sqrt {D}}}[\cos \langle w_{1},x\rangle ,\sin \langle w_{1},x\rangle ,\cdots \cos \langle w_{D},x\rangle ,\sin \langle w_{D},x\rangle ]^{T}}$ 여기서 $\displaystyle w_{1},...,w_{D}}$는 정규 분포 $\displaystyle N(0,\sigma ^{2}I)}$에서 독립적으로 샘플링된 것이다. 이러한 매개변수 선택은 $\displaystyle \mathbb {E} [\langle \varphi (x),\varphi (y)\rangle ]=e^{-{\frac {\|x-y\|^{2}}{2\sigma ^{2}}}}}$ 또는 다음과 같이 만족한다. $\displaystyle e^{\langle x,y\rangle /\sigma ^{2}}=\mathbb {E} [\langle e^{\|x\|^{2}/2\sigma ^{2}}\varphi (x),e^{\|y\|^{2}/2\sigma ^{2}}\varphi (y)\rangle ]\approx \langle e^{\|x\|^{2}/2\sigma ^{2}}\varphi (x),e^{\|y\|^{2}/2\sigma ^{2}}\varphi (y)\rangle }$ 결과적으로, 하나의 쿼리를 가진 단일 헤드 어텐션은 다음과 같이 쓸 수 있다. $\displaystyle {\text{Attention}}(q,K,V)={\text{softmax}}\left({\frac {qK^{\mathrm {T} }}{\sqrt {d_{k}}}}\right)V\approx {\frac {\varphi (q)^{T}\sum _{i}e^{\|k_{i}\|^{2}/2\sigma ^{2}}\varphi (k_{i})v_{i}^{T}}{\varphi (q)^{T}\sum _{i}e^{\|k_{i}\|^{2}/2\sigma ^{2}}\varphi (k_{i})}}}$ 여기서 $\displaystyle \sigma =d_{K}^{1/4}}$이다. 여러 쿼리 및 멀티헤드 어텐션도 마찬가지이다.

이 근사치는 선형 시간으로 계산될 수 있는데, 이는 행렬 $\displaystyle \varphi (k_{i})v_{i}^{T}}$를 먼저 계산한 다음 쿼리와 곱할 수 있기 때문이다. 본질적으로 다음과 같은 더 정확한 버전을 얻을 수 있었다. $\displaystyle {\text{Attention}}(Q,K,V)={\text{softmax}}\left({\frac {QK^{\mathrm {T} }}{\sqrt {d_{k}}}}\right)V\approx Q(K^{T}V/{\sqrt {d_{k}}})}$ 퍼포머(Performer) (2022)는 동일한 랜덤 특징 어텐션을 사용하지만, $\displaystyle w_{1},...,w_{D}}$는 먼저 정규 분포 $\displaystyle N(0,\sigma ^{2}I)}$에서 독립적으로 샘플링된 다음, [그람-슈미트 처리](https://ko.wikipedia.org/wiki/%EA%B7%B8%EB%9E%8C-%EC%8A%88%EB%AF%B8%ED%8A%B8_%EA%B3%BC%EC%A0%95)된다.

#### 멀티모달리티

트랜스포머는 텍스트를 넘어선 모달리티(입력 또는 출력)에도 사용/적용될 수 있으며, 일반적으로 모달리티를 "토큰화"하는 방법을 찾는 방식으로 이루어진다.

멀티모달 모델은 처음부터 훈련되거나 미세 조정을 통해 훈련될 수 있다. 2022년 연구에서는 자연어만으로 사전 훈련된 트랜스포머를 매개변수의 0.03%만으로 미세 조정하여 다양한 논리 및 시각 작업에서 LSTM과 경쟁할 수 있음을 발견하여 [전이학습](https://ko.wikipedia.org/wiki/%EC%A0%84%EC%9D%B4%ED%95%99%EC%8A%B5)을 입증했다. LLaVA는 언어 모델(Vicuna-13B)과 비전 모델([ViT](https://ko.wikipedia.org/wiki/%EB%B9%84%EC%A0%84_%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8)-L/14)로 구성된 시각-언어 모델이며, 선형 계층으로 연결된다. 선형 계층만 미세 조정된다.

[비전 트랜스포머](https://ko.wikipedia.org/wiki/%EB%B9%84%EC%A0%84_%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8)는 입력 이미지를 일련의 패치로 분해하고, 이를 벡터로 변환하여 표준 트랜스포머의 토큰처럼 처리함으로써 트랜스포머를 컴퓨터 비전에 적용한다.

컨포머(Conformer)와 이후의 [휘스퍼](https://ko.wikipedia.org/w/index.php?title=%ED%9C%98%EC%8A%A4%ED%8D%BC_(%EC%9D%8C%EC%84%B1_%EC%9D%B8%EC%8B%9D_%EC%8B%9C%EC%8A%A4%ED%85%9C)&action=edit&redlink=1)는 [음성 인식](https://ko.wikipedia.org/wiki/%EC%9D%8C%EC%84%B1_%EC%9D%B8%EC%8B%9D)에 대해 동일한 패턴을 따르며, 먼저 음성 신호를 [스펙트로그램](https://ko.wikipedia.org/wiki/%EC%8A%A4%ED%8E%99%ED%8A%B8%EB%A1%9C%EA%B7%B8%EB%9E%A8)으로 변환한 다음, 이를 이미지처럼 처리한다. 즉, 일련의 패치로 분해하고 벡터로 변환하여 표준 트랜스포머의 토큰처럼 처리한다.

[퍼시버](https://ko.wikipedia.org/w/index.php?title=%ED%8D%BC%EC%8B%9C%EB%B2%84&action=edit&redlink=1)는 멀티모달리티를 위해 설계된 트랜스포머의 변형이다.

이미지 생성을 위한 주목할 만한 아키텍처로는 [DALL-E 1](https://ko.wikipedia.org/wiki/DALL-E) (2021), Parti (2022), Phenaki (2023), 및 Muse (2023)가 있다. 이후 모델과 달리 DALL-E는 확산 모델이 아니다. 대신, 자기회귀적으로 텍스트를 생성한 다음 이미지의 토큰 표현으로 변환하고, 이를 [변분 오토인코더](https://ko.wikipedia.org/wiki/%EB%B3%80%EB%B6%84_%EC%98%A4%ED%86%A0%EC%9D%B8%EC%BD%94%EB%8D%94)로 이미지로 변환하는 디코더-온리 트랜스포머를 사용한다. Parti는 인코더-디코더 트랜스포머로, 인코더는 텍스트 프롬프트를 처리하고 디코더는 이미지의 토큰 표현을 생성한다. Muse는 마스킹되지 않은 이미지 토큰으로부터 마스킹된 이미지 토큰을 예측하도록 훈련된 인코더-온리 트랜스포머이다. 생성 중에 모든 입력 토큰은 마스킹되며, 가장 높은 신뢰도의 예측은 다음 반복에 포함되어 모든 토큰이 예측될 때까지 계속된다. Phenaki는 텍스트-비디오 모델이다. 이는 사전 계산된 텍스트 토큰을 조건으로 하는 양방향 마스크드 트랜스포머이다. 생성된 토큰은 비디오로 디코딩된다.

### 응용 분야

트랜스포머는 [자연어 처리](https://ko.wikipedia.org/wiki/%EC%9E%90%EC%97%B0%EC%96%B4_%EC%B2%98%EB%A6%AC) (NLP) 분야에서 큰 성공을 거두었다. [GPT-2](https://ko.wikipedia.org/wiki/GPT-2), [GPT-3](https://ko.wikipedia.org/wiki/GPT-3), [GPT-4](https://ko.wikipedia.org/wiki/GPT-4), [제미나이](https://ko.wikipedia.org/wiki/%EC%A0%9C%EB%AF%B8%EB%82%98%EC%9D%B4_(%EC%B1%97%EB%B4%87)), AlbertAGPT, [클로드](https://ko.wikipedia.org/wiki/%EC%95%A4%ED%8A%B8%EB%A1%9C%ED%94%BD#클로드), [BERT](https://ko.wikipedia.org/wiki/BERT_(%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8)), [그록](https://ko.wikipedia.org/wiki/Grok), [XLNet](https://ko.wikipedia.org/w/index.php?title=XLNet&action=edit&redlink=1), [RoBERTa](https://ko.wikipedia.org/wiki/BERT_(%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8)#RoBERTa) 및 [챗GPT](https://ko.wikipedia.org/wiki/%EC%B1%97GPT)와 같은 많은 [대형 언어 모델](https://ko.wikipedia.org/wiki/%EB%8C%80%ED%98%95_%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8)은 다양한 NLP 관련 하위 작업 및 관련 실제 응용 프로그램에서 트랜스포머의 능력을 보여주었다. 다음을 포함한다.

- 기계 번역
- 시계열 예측
- 문서 요약
- 문서 생성
- 개체명 인식 (NER)
- 자연어로 표현된 요구 사항을 기반으로 컴퓨터 코드 작성.
- 음성 인식

전통적인 NLP 외에도 트랜스포머 아키텍처는 다음과 같은 다른 응용 분야에서 성공을 거두었다.

- 생물학적 서열 분석
- 비디오 이해
- 단백질 구조 예측 (알파폴드 등)
- 체스판 위치 평가. 정적 평가만 사용했을 때 (즉, 최소극대화 탐색 없이) 트랜스포머는 엘로 2895점을 달성하여 그랜드마스터 수준에 도달했다.

### 같이 보기

- Seq2seq
- 퍼시버
- 비전 트랜스포머
- 대형 언어 모델
- BERT (언어 모델)
- GPT (언어 모델)
- T5 (언어 모델)

### 참고 문헌

- Alexander Rush, The Annotated transformer 보관됨 2021-09-22 - 웨이백 머신, Harvard NLP group, 3 April 2018
- Phuong, Mary; Hutter, Marcus (2022). “Formal Algorithms for Transformers”. arXiv:2207.09238 [cs.LG]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
- Ferrando, Javier; Sarti, Gabriele; Bisazza, Arianna; Costa-jussà, Marta R. (2024년 5월 1일). “A Primer on the Inner Workings of Transformer-based Language Models”. arXiv:2405.00208 [cs.CL]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
- Leech, Gavin (2024년 11월 6일). “Transformer++”. 《argmin gravitas》. 2025년 2월 26일에 원본 문서에서 보존된 문서. 2025년 5월 8일에 확인함.

### 각주

1. ↑ 가 나 다 라 마 바 사 아 자 차 카 타 Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Łukasz; Polosukhin, Illia (2017). 《Attention is All you Need》 (PDF). 《Advances in Neural Information Processing Systems》 30 (Curran Associates, Inc.).
2. ↑ Hochreiter, Sepp; Schmidhuber, Jürgen (1997년 11월 1일). 《Long Short-Term Memory》. 《Neural Computation》 9. 1735–1780쪽. doi:10.1162/neco.1997.9.8.1735. ISSN 0899-7667. PMID 9377276. S2CID 1915014.
3. ↑ 가 나 “Better Language Models and Their Implications”. 《OpenAI》. 2019년 2월 14일. 2020년 12월 19일에 원본 문서에서 보존된 문서. 2019년 8월 25일에 확인함.
4. ↑ 가 나 Bahdanau; Cho, Kyunghyun; Bengio, Yoshua (2014년 9월 1일). “Neural Machine Translation by Jointly Learning to Align and Translate”. arXiv:1409.0473 [cs.CL]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
5. ↑ Luong, Minh-Thang; Pham, Hieu; Manning, Christopher D. (2015년 8월 17일). “Effective Approaches to Attention-based Neural Machine Translation”. arXiv:1508.04025 [cs.CL]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
6. ↑ 가 나 Chen, Lili; Lu, Kevin; Rajeswaran, Aravind; Lee, Kimin; Grover, Aditya; Laskin, Michael; Abbeel, Pieter; Srinivas, Aravind; Mordatch, Igor (2021년 6월 24일), 《Decision Transformer: Reinforcement Learning via Sequence Modeling》, arXiv:2106.01345
7. ↑ Parisotto, Emilio; Song, Francis; Rae, Jack; Pascanu, Razvan; Gulcehre, Caglar; Jayakumar, Siddhant; Jaderberg, Max; Kaufman, Raphaël Lopez; Clark, Aidan; Noury, Seb; Botvinick, Matthew; Heess, Nicolas; Hadsell, Raia (2020년 11월 21일). 《Stabilizing Transformers for Reinforcement Learning》. 《Proceedings of the 37th International Conference on Machine Learning》 (영어) (PMLR). 7487–7498쪽.
8. ↑ Radford, Alec; Jong Wook Kim; Xu, Tao; Brockman, Greg; McLeavey, Christine; Sutskever, Ilya (2022). “Robust Speech Recognition via Large-Scale Weak Supervision”. arXiv:2212.04356 [eess.AS]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
9. ↑ Monastirsky, Maxim; Azulay, Osher; Sintov, Avishai (February 2023). 《Learning to Throw With a Handful of Samples Using Decision Transformers》. 《IEEE Robotics and Automation Letters》 8. 576–583쪽. doi:10.1109/LRA.2022.3229266. ISSN 2377-3766.
10. ↑ 가 나 Ruoss, Anian; Delétang, Grégoire; Medapati, Sourabh; Grau-Moya, Jordi; Wenliang, Li; Catt, Elliot; Reid, John; Genewein, Tim (2024년 2월 7일). “Grandmaster-Level Chess Without Search”. arXiv:2402.04494v1 [cs.LG]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
11. ↑ 가 나 Wolf, Thomas; Debut, Lysandre; Sanh, Victor; Chaumond, Julien; Delangue, Clement; Moi, Anthony; Cistac, Pierric; Rault, Tim; Louf, Remi; Funtowicz, Morgan; Davison, Joe; Sam; von Platen, Patrick; Ma, Clara; Jernite, Yacine; Plu, Julien; Xu, Canwen; Le Scao, Teven; Gugger, Sylvain; Drame, Mariama; Lhoest, Quentin; Rush, Alexander (2020). 〈Transformers: State-of-the-Art Natural Language Processing〉. 《Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations》. 38–45쪽. doi:10.18653/v1/2020.emnlp-demos.6. S2CID 208117506.
12. ↑ 가 나 다 “Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing”. 《Google AI Blog》. 2018년 11월 2일. 2021년 1월 13일에 원본 문서에서 보존된 문서. 2019년 8월 25일에 확인함.
13. ↑ Feldman, J. A.; Ballard, D. H. (1982년 7월 1일). 《Connectionist models and their properties》. 《Cognitive Science》 6. 205–254쪽. doi:10.1016/S0364-0213(82)80001-3. ISSN 0364-0213.
14. ↑ Rumelhart, David E.; McClelland, James L.; Hinton, Geoffrey E. (1987년 7월 29일). 《Parallel Distributed Processing, Volume 1: Explorations in the Microstructure of Cognition: Foundations, Chapter 2》 (PDF) (영어). Cambridge, Mass: Bradford Books. ISBN 978-0-262-68053-0.
15. ↑ Giles, C. Lee; Maxwell, Tom (1987년 12월 1일). 《Learning, invariance, and generalization in high-order neural networks》. 《Applied Optics》 (영어) 26. 4972–4978쪽. doi:10.1364/AO.26.004972. ISSN 0003-6935. PMID 20523475.
16. ↑ 가 나 Schmidhuber, Jürgen (1992). 《Learning to control fast-weight memories: an alternative to recurrent nets.》 (PDF). 《Neural Computation》 4. 131–139쪽. doi:10.1162/neco.1992.4.1.131. S2CID 16683347.
17. ↑ Christoph von der Malsburg: The correlation theory of brain function. Internal Report 81-2, MPI Biophysical Chemistry, 1981. http://cogprints.org/1380/1/vdM_correlation.pdf See Reprint in Models of Neural Networks II, chapter 2, pages 95–119. Springer, Berlin, 1994.
18. ↑ Jerome A. Feldman, "Dynamic connections in neural networks," Biological Cybernetics, vol. 46, no. 1, pp. 27–39, Dec. 1982.
19. ↑ Hinton, Geoffrey E.; Plaut, David C. (1987). 《Using Fast Weights to Deblur Old Memories》. 《Proceedings of the Annual Meeting of the Cognitive Science Society》 (영어) 9.
20. ↑ Katharopoulos, Angelos; Vyas, Apoorv; Pappas, Nikolaos; Fleuret, François (2020). 〈Transformers are RNNs: Fast autoregressive Transformers with linear attention〉. 《ICML 2020》. PMLR. 5156–5165쪽.
21. ↑ Schlag, Imanol; Irie, Kazuki; Schmidhuber, Jürgen (2021). 〈Linear Transformers Are Secretly Fast Weight Programmers〉. 《ICML 2021》. Springer. 9355–9366쪽.
22. ↑ 가 나 Cho, Kyunghyun; van Merriënboer, Bart; Gulcehre, Caglar; Bahdanau, Dzmitry; Bougares, Fethi; Schwenk, Holger; Bengio, Yoshua (October 2014). 〈Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation〉. Moschitti, Alessandro; Pang, Bo; Daelemans, Walter. 《Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)》. Doha, Qatar: Association for Computational Linguistics. 1724–1734쪽. arXiv:1406.1078. doi:10.3115/v1/D14-1179.
23. ↑ 가 나 Sutskever, Ilya; Vinyals, Oriol; Le, Quoc Viet (2014년 12월 14일). “Sequence to sequence learning with neural networks”. arXiv:1409.3215 [cs.CL]. 더 이상 지원되지 않는 변수를 사용함 (도움말) [first version posted to arXiv on 10 Sep 2014]
24. ↑ Chung, Junyoung; Gulcehre, Caglar; Cho, KyungHyun; Bengio, Yoshua (2014). “Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling”. arXiv:1412.3555 [cs.NE]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
25. ↑ Gruber, N.; Jockisch, A. (2020), “Are GRU cells more specific and LSTM cells more sensitive in motive classification of text?”, 《Frontiers in Artificial Intelligence》 3: 40, doi:10.3389/frai.2020.00040, PMC 7861254, PMID 33733157, S2CID 220252321
26. ↑ Sutskever, Ilya; Vinyals, Oriol; Le, Quoc V (2014). 《Sequence to Sequence Learning with Neural Networks》. 《Advances in Neural Information Processing Systems》 27 (Curran Associates, Inc.). arXiv:1409.3215. 2025년 1월 27일에 원본 문서에서 보존된 문서. 2025년 6월 20일에 확인함.
27. ↑ Luong, Minh-Thang; Pham, Hieu; Manning, Christopher D. (2015). “Effective Approaches to Attention-based Neural Machine Translation”. arXiv:1508.04025 [cs.CL]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
28. ↑ Wu, Yonghui; 외. (2016년 9월 1일). “Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation”. arXiv:1609.08144 [cs.CL]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
29. ↑ Lewis-Kraus, Gideon (2016년 12월 14일). “The Great A.I. Awakening”. 《The New York Times》. ISSN 0362-4331. 2023년 5월 24일에 원본 문서에서 보존된 문서. 2023년 6월 22일에 확인함.
30. ↑ Parikh, Ankur P.; Täckström, Oscar; Das, Dipanjan; Uszkoreit, Jakob (2016년 9월 25일). “A Decomposable Attention Model for Natural Language Inference”. arXiv:1606.01933 [cs.CL]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
31. ↑ 가 나 Levy, Steven. “8 Google Employees Invented Modern AI. Here's the Inside Story”. 《Wired》 (미국 영어). ISSN 1059-1028. 2024년 3월 20일에 원본 문서에서 보존된 문서. 2024년 8월 6일에 확인함.
32. ↑ Cheng, Jianpeng; Dong, Li; Lapata, Mirella (November 2016). 〈Long Short-Term Memory-Networks for Machine Reading〉. Su, Jian; Duh, Kevin; Carreras, Xavier. 《Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing》. Austin, Texas: Association for Computational Linguistics. 551–561쪽. doi:10.18653/v1/D16-1053.
33. ↑ Peng, Bo; Alcaide, Eric; Anthony, Quentin; Albalak, Alon; Arcadinho, Samuel; Biderman, Stella; Cao, Huanqi; Cheng, Xin; Chung, Michael (2023년 12월 10일), 《RWKV: Reinventing RNNs for the Transformer Era》, arXiv:2305.13048
34. ↑ Marche, Stephen (2024년 8월 23일). “Was Linguistic A.I. Created by Accident?”. 《The New Yorker》 (미국 영어). ISSN 0028-792X. 2024년 8월 27일에 확인함.
35. ↑ 가 나 다 라 마 바 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018년 10월 11일). “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”. arXiv:1810.04805v2 [cs.CL]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
36. ↑ “Google: BERT now used on almost every English query”. 《Search Engine Land》. 2020년 10월 15일. 2020년 11월 24일에 확인함.
37. ↑ “Recent Advances in Google Translate”. 《research.google》 (영어). 2024년 5월 8일에 확인함.
38. ↑ “The inside story of how ChatGPT was built from the people who made it”. 《MIT Technology Review》 (영어). 2024년 8월 6일에 확인함.
39. ↑ “Improving language understanding with unsupervised learning”. 《openai.com》. 2018년 6월 11일. 2023년 3월 18일에 원본 문서에서 보존된 문서. 2023년 3월 18일에 확인함.
40. ↑ 《finetune-transformer-lm》, OpenAI, 2018년 6월 11일, 2023년 5월 1일에 확인함
41. ↑ 가 나 Dosovitskiy, Alexey; Beyer, Lucas; Kolesnikov, Alexander; Weissenborn, Dirk; Zhai, Xiaohua; Unterthiner, Thomas; Dehghani, Mostafa; Minderer, Matthias; Heigold, Georg; Gelly, Sylvain; Uszkoreit, Jakob (2021년 6월 3일). “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”. arXiv:2010.11929 [cs.CV]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
42. ↑ 가 나 Gulati, Anmol; Qin, James; Chiu, Chung-Cheng; Parmar, Niki; Zhang, Yu; Yu; Han, Wei; Wang, Shibo; Zhang, Zhengdong; Wu, Yonghui; Pang, Ruoming (2020). “Conformer: Convolution-augmented Transformer for Speech Recognition”. arXiv:2005.08100 [eess.AS]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
43. ↑ Choromanski, Krzysztof; Likhosherstov, Valerii; Dohan, David; Song, Xingyou; Gane, Andreea; Sarlos, Tamas; Hawkins, Peter; Davis, Jared; Mohiuddin, Afroz (2022년 11월 19일), 《Rethinking Attention with Performers》, arXiv:2009.14794
44. ↑ Liu, Zhuang; Mao, Hanzi; Wu, Chao-Yuan; Feichtenhofer, Christoph; Darrell, Trevor; Xie, Saining (2022). 《A ConvNet for the 2020s》. Conference on Computer Vision and Pattern Recognition (영어). 11976–11986쪽.
45. ↑ Esser, Patrick; Kulal, Sumith; Blattmann, Andreas; Entezari, Rahim; Müller, Jonas; Saini, Harry; Levi, Yam; Lorenz, Dominik; Sauer, Axel (2024년 3월 5일), 《Scaling Rectified Flow Transformers for High-Resolution Image Synthesis》, arXiv:2403.03206
46. ↑ 가 나 Xiong, Ruibin; Yang, Yunchang; He, Di; Zheng, Kai; Zheng, Shuxin; Xing, Chen; Zhang, Huishuai; Lan, Yanyan; Wang, Liwei; Liu, Tie-Yan (2020년 6월 29일). “On Layer Normalization in the Transformer Architecture”. arXiv:2002.04745 [cs.LG]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
47. ↑ Raffel, Colin; Shazeer, Noam; Roberts, Adam; Lee, Katherine; Narang, Sharan; Matena, Michael; Zhou, Yanqi; Li, Wei; Liu, Peter J. (2020년 1월 1일). 《Exploring the limits of transfer learning with a unified text-to-text transformer》. 《The Journal of Machine Learning Research》 21. 140:5485–140:5551쪽. arXiv:1910.10683. ISSN 1532-4435.
48. ↑ Raffel, Colin; Shazeer, Noam; Roberts, Adam; Lee, Katherine; Narang, Sharan; Matena, Michael; Zhou, Yanqi; Li, Wei; Liu, Peter J. (2019). “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer”. arXiv:1910.10683 [cs.LG]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
49. ↑ 가 나 “Masked language modeling”. 《huggingface.co》. 2023년 10월 5일에 확인함.
50. ↑ 가 나 “Causal language modeling”. 《huggingface.co》. 2023년 10월 5일에 확인함.
51. ↑ 가 나 다 라 Tay, Yi; Dehghani, Mostafa; Tran, Vinh Q.; Garcia, Xavier; Wei, Jason; Wang, Xuezhi; Chung, Hyung Won; Shakeri, Siamak; Bahri, Dara (2023년 2월 28일), 《UL2: Unifying Language Learning Paradigms》, arXiv:2205.05131
52. ↑ Press, Ofir; Wolf, Lior (2017년 2월 21일), 《Using the Output Embedding to Improve Language Models》, arXiv:1608.05859
53. ↑ Lintz, Nathan (2016년 4월 18일). “Sequence Modeling with Neural Networks (Part 2): Attention Models”. 《Indico》. 2020년 10월 21일에 원본 문서에서 보존된 문서. 2019년 10월 15일에 확인함.
54. ↑ 가 나 다 Alammar, Jay. “The Illustrated Transformer”. 《jalammar.github.io》. 2020년 10월 18일에 원본 문서에서 보존된 문서. 2019년 10월 15일에 확인함.
55. ↑ Team, Keras. “Keras documentation: GPT2Backbone model”. 《keras.io》 (영어). 2024년 8월 8일에 확인함.
56. ↑ Clark, Kevin; Khandelwal, Urvashi; Levy, Omer; Manning, Christopher D. (August 2019). 《What Does BERT Look at? An Analysis of BERT's Attention》. 《Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP》 (Florence, Italy: Association for Computational Linguistics). 276–286쪽. arXiv:1906.04341. doi:10.18653/v1/W19-4828. 2020년 10월 21일에 원본 문서에서 보존된 문서. 2020년 5월 20일에 확인함.
57. ↑ Yang, Zhilin; Dai, Zihang; Yang, Yiming; Carbonell, Jaime; Salakhutdinov, Russ R; Le, Quoc V (2019). 《XLNet: Generalized Autoregressive Pretraining for Language Understanding》. 《Advances in Neural Information Processing Systems》 32 (Curran Associates, Inc.). arXiv:1906.08237.
58. ↑ Radford, Alec; Narasimhan, Karthik; Salimans, Tim; Sutskever, Ilya (2018년 6월 11일). “Improving Language Understanding by Generative Pre-Training” (PDF). 오픈AI. 12쪽. 2021년 1월 26일에 원본 문서 (PDF)에서 보존된 문서. 2021년 1월 23일에 확인함.
59. ↑ Wang, Qiang; Li, Bei; Xiao, Tong; Zhu, Jingbo; Li, Changliang; Wong, Derek F.; Chao, Lidia S. (2019년 6월 4일), 《Learning Deep Transformer Models for Machine Translation》, arXiv:1906.01787
60. ↑ Phuong, Mary; Hutter, Marcus (2022년 7월 19일), 《Formal Algorithms for Transformers》, arXiv:2207.09238
61. ↑ 가 나 다 Raffel, Colin; Shazeer, Noam; Roberts, Adam; Lee, Katherine; Narang, Sharan; Matena, Michael; Zhou, Yanqi; Li, Wei; Liu, Peter J. (2020). 《Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer》. 《Journal of Machine Learning Research》 21. 1–67쪽. arXiv:1910.10683. ISSN 1533-7928.
62. ↑ “Recent Advances in Google Translate”. 《Google Research》 (영어). 2020년 6월 8일. 2024년 7월 4일에 원본 문서에서 보존된 문서. 2024년 8월 7일에 확인함.
63. ↑ 가 나 Shazeer, Noam (2020년 2월 1일). “GLU Variants Improve Transformer”. arXiv:2002.05202 [cs.LG]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
64. ↑ Hendrycks, Dan; Gimpel, Kevin (2016년 6월 27일). “Gaussian Error Linear Units (GELUs)” (영어). arXiv:1606.08415v5 [cs.LG]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
65. ↑ Zhang, Biao; Sennrich, Rico (2019). 《Root Mean Square Layer Normalization》. 《Advances in Neural Information Processing Systems》 32 (Curran Associates, Inc.). arXiv:1910.07467.
66. ↑ Tembine, Hamidou, Manzoor Ahmed Khan, and Issa Bamia. 2024. "Mean-Field-Type Transformers" Mathematics 12, no. 22: 3506. https://doi.org/10.3390/math12223506
67. ↑ 가 나 Nguyen, Toan Q.; Salazar, Julian (2019년 11월 2일). Niehues, Jan; Cattoni, Rolando; Stüker, Sebastian; Negri, Matteo; Turchi, Marco; Ha, Thanh-Le; Salesky, Elizabeth; Sanabria, Ramon; Barrault, Loic, 편집. 《Transformers without Tears: Improving the Normalization of Self-Attention》. 《Proceedings of the 16th International Conference on Spoken Language Translation》 (Hong Kong: Association for Computational Linguistics). arXiv:1910.05895. doi:10.5281/zenodo.3525484.
68. ↑ Dufter, Philipp; Schmitt, Martin; Schütze, Hinrich (2022년 6월 6일). 《Position Information in Transformers: An Overview》. 《Computational Linguistics》 48. 733–763쪽. arXiv:2102.11090. doi:10.1162/coli_a_00445. ISSN 0891-2017. S2CID 231986066.
69. ↑ Gehring, Jonas; Auli, Michael; Grangier, David; Yarats, Denis; Dauphin, Yann N. (2017년 7월 17일). 《Convolutional Sequence to Sequence Learning》. 《Proceedings of the 34th International Conference on Machine Learning》 (영어) (PMLR). 1243–1252쪽.
70. ↑ Haviv, Adi; Ram, Ori; Press, Ofir; Izsak, Peter; Levy, Omer (2022년 12월 5일), 《Transformer Language Models without Positional Encodings Still Learn Positional Information》, arXiv:2203.16634
71. ↑ Su, Jianlin; Lu, Yu; Pan, Shengfeng; Murtadha, Ahmed; Wen, Bo; Liu, Yunfeng (2021년 4월 1일). “RoFormer: Enhanced Transformer with Rotary Position Embedding”. arXiv:2104.09864 [cs.CL]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
72. ↑ Press, Ofir; Smith, Noah A.; Lewis, Mike (2021년 8월 1일). “Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation”. arXiv:2108.12409 [cs.CL]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
73. ↑ Shaw, Peter; Uszkoreit, Jakob; Vaswani, Ashish (2018). “Self-Attention with Relative Position Representations”. arXiv:1803.02155 [cs.CL]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
74. ↑ Ke, Guolin; He, Di; Liu, Tie-Yan (2021년 3월 15일), 《Rethinking Positional Encoding in Language Pre-training》, arXiv:2006.15595
75. ↑ Kwon, Woosuk; Li, Zhuohan; Zhuang, Siyuan; Sheng, Ying; Zheng, Lianmin; Yu, Cody Hao; Gonzalez, Joseph; Zhang, Hao; Stoica, Ion (2023년 10월 23일). 〈Efficient Memory Management for Large Language Model Serving with PagedAttention〉. SOSP '23. New York, NY, USA: Association for Computing Machinery. 611–626쪽. arXiv:2309.06180. doi:10.1145/3600006.3613165. ISBN 979-8-4007-0229-7. |제목=이(가) 없거나 비었음 (도움말)
76. ↑ 《vllm-project/vllm》, vLLM, 2024년 6월 20일, 2024년 6월 20일에 확인함
77. ↑ Contribution), Woosuk Kwon*, Zhuohan Li*, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Yu, Joey Gonzalez, Hao Zhang, and Ion Stoica (* Equal (2023년 6월 20일). “vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention”. 《vLLM Blog》 (영어). 2024년 6월 20일에 확인함.
78. ↑ Dao, Tri; Fu, Dan; Ermon, Stefano; Rudra, Atri; Ré, Christopher (2022년 12월 6일). 《FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness》. 《Advances in Neural Information Processing Systems》 35. 16344–16359쪽. arXiv:2205.14135.
79. ↑ “Stanford CRFM”. 《crfm.stanford.edu》. 2023년 7월 18일에 확인함.
80. ↑ “FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning”. 《Princeton NLP》. 2023년 6월 17일. 2023년 7월 18일에 확인함.
81. ↑ “Introducing Together AI Chief Scientist Tri Dao, as he releases FlashAttention-2 to speed up model training and inference”. 《TOGETHER》. 2023년 7월 18일에 확인함.
82. ↑ Ainslie, Joshua; Lee-Thorp, James; de Jong, Michiel; Zemlyanskiy, Yury; Lebrón, Federico; Sanghai, Sumit (2023년 12월 23일). “GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints”. arXiv:2305.13245 [cs.CL]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
83. ↑ Chowdhery, Aakanksha; Narang, Sharan; Devlin, Jacob; Bosma, Maarten; Mishra, Gaurav; Roberts, Adam; Barham, Paul; Chung, Hyung Won; Sutton, Charles; Gehrmann, Sebastian; Schuh, Parker; Shi, Kensen; Tsvyashchenko, Sasha; Maynez, Joshua; Rao, Abhishek (2022년 4월 1일). “PaLM: Scaling Language Modeling with Pathways”. arXiv:2204.02311 [cs.CL]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
84. ↑ Ainslie, Joshua; Lee-Thorp, James; de Jong, Michiel; Zemlyanskiy, Yury; Lebrón, Federico; Sanghai, Sumit (2023년 12월 23일), 《GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints》, arXiv:2305.13245
85. ↑ 가 나 DeepSeek-AI; Liu, Aixin; Feng, Bei; Wang, Bin; Wang, Bingxuan; Liu, Bo; Zhao, Chenggang; Dengr, Chengqi; Ruan, Chong (2024년 6월 19일), 《DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model》, arXiv:2405.04434 .
86. ↑ 가 나 Leviathan, Yaniv; Kalman, Matan; Matias, Yossi (2023년 5월 18일), 《Fast Inference from Transformers via Speculative Decoding》, arXiv:2211.17192
87. ↑ Fu, Yao (2023년 12월 13일). “Towards 100x Speedup: Full Stack Transformer Inference Optimization”.
88. ↑ Chen, Charlie; Borgeaud, Sebastian; Irving, Geoffrey; Lespiau, Jean-Baptiste; Sifre, Laurent; Jumper, John (2023년 2월 2일), 《Accelerating Large Language Model Decoding with Speculative Sampling》, arXiv:2302.01318
89. ↑ Gloeckle, Fabian; Badr Youbi Idrissi; Rozière, Baptiste; Lopez-Paz, David; Synnaeve, Gabriel (2024). “Better & Faster Large Language Models via Multi-token Prediction”. arXiv:2404.19737 [cs.CL]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
90. ↑ DeepSeek-AI; 외. (2024). “DeepSeek-V3 Technical Report”. arXiv:2412.19437 [cs.CL]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
91. ↑ 가 나 Kitaev, Nikita; Kaiser, Łukasz; Levskaya, Anselm (2020). “Reformer: The Efficient Transformer”. arXiv:2001.04451 [cs.LG]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
92. ↑ Liu, Ze; Lin, Yutong; Cao, Yue; Hu, Han; Wei, Yixuan; Zhang, Zheng; Lin, Stephen; Guo, Baining (2021). 〈Swin Transformer: Hierarchical Vision Transformer using Shifted Windows〉. 《2021 IEEE/CVF International Conference on Computer Vision (ICCV)》. IEEE. 9992–10002쪽. arXiv:2103.14030. doi:10.1109/ICCV48922.2021.00986. ISBN 978-1-6654-2812-5.
93. ↑ Ristea, Nicolaea Catalin; Ionescu, Radu Tudor; Khan, Fahad Shahbaz (2022년 9월 18일). 《SepTr: Separable Transformer for Audio Spectrogram Processing》. 《Interspeech》 (영어) (ISCA). 4103–4107쪽. arXiv:2203.09581. doi:10.21437/Interspeech.2022-249.
94. ↑ Tay, Yi; Dehghani, Mostafa; Abnar, Samira; Shen, Yikang; Bahri, Dara; Pham, Philip; Rao, Jinfeng; Yang, Liu; Ruder, Sebastian; Metzler, Donald (2020년 11월 8일). “Long Range Arena: A Benchmark for Efficient Transformers”. arXiv:2011.04006 [cs.LG]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
95. ↑ “Reformer: The Efficient Transformer”. 《Google AI Blog》. 2020년 1월 16일. 2020년 10월 22일에 원본 문서에서 보존된 문서. 2020년 10월 22일에 확인함.
96. ↑ Gomez, Aidan N; Ren, Mengye; Urtasun, Raquel; Grosse, Roger B (2017). 《The Reversible Residual Network: Backpropagation Without Storing Activations》. 《Advances in Neural Information Processing Systems》 30 (Curran Associates, Inc.). arXiv:1707.04585.
97. ↑ Child, Rewon; Gray, Scott; Radford, Alec; Sutskever, Ilya (2019년 4월 23일), 《Generating Long Sequences with Sparse Transformers》, arXiv:1904.10509
98. ↑ “Constructing Transformers For Longer Sequences with Sparse Attention Methods”. 《Google AI Blog》. 2021년 3월 25일. 2021년 9월 18일에 원본 문서에서 보존된 문서. 2021년 5월 28일에 확인함.
99. ↑ Zhai, Shuangfei; Talbott, Walter; Srivastava, Nitish; Huang, Chen; Goh, Hanlin; Zhang, Ruixiang; Susskind, Josh (2021년 9월 21일). “An Attention Free Transformer”. arXiv:2105.14103 [cs.LG]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
100. ↑ Peng, Hao; Pappas, Nikolaos; Yogatama, Dani; Schwartz, Roy; Smith, Noah A.; Kong, Lingpeng (2021년 3월 19일). “Random Feature Attention”. arXiv:2103.02143 [cs.CL]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
101. ↑ Choromanski, Krzysztof; Likhosherstov, Valerii; Dohan, David; Song, Xingyou; Gane, Andreea; Sarlos, Tamas; Hawkins, Peter; Davis, Jared; Belanger, David; Colwell, Lucy; Weller, Adrian (2020년 9월 30일). “Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers”. arXiv:2006.03555 [cs.LG]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
102. ↑ Lu, Kevin; Grover, Aditya; Abbeel, Pieter; Mordatch, Igor (2022년 6월 28일). 《Frozen Pretrained Transformers as Universal Computation Engines》. 《Proceedings of the AAAI Conference on Artificial Intelligence》 (영어) 36. 7628–7636쪽. doi:10.1609/aaai.v36i7.20729. ISSN 2374-3468.
103. ↑ “Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality | LMSYS Org”. 《lmsys.org》 (영어). 2024년 8월 11일에 확인함.
104. ↑ Liu, Haotian; Li, Chunyuan; Wu, Qingyang; Lee, Yong Jae (2023년 12월 15일). 《Visual Instruction Tuning》. 《Advances in Neural Information Processing Systems》 (영어) 36. 34892–34916쪽.
105. ↑ Radford, Alec; Kim, Jong Wook; Xu, Tao; Brockman, Greg; McLeavey, Christine; Sutskever, Ilya (2022). “Robust Speech Recognition via Large-Scale Weak Supervision”. arXiv:2212.04356 [eess.AS]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
106. ↑ Jaegle, Andrew; Gimeno, Felix; Brock, Andrew; Zisserman, Andrew; Vinyals, Oriol; Carreira, Joao (2021년 6월 22일). “Perceiver: General Perception with Iterative Attention”. arXiv:2103.03206 [cs.CV]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
107. ↑ Jaegle, Andrew; Borgeaud, Sebastian; Alayrac, Jean-Baptiste; Doersch, Carl; Ionescu, Catalin; Ding, David; Koppula, Skanda; Zoran, Daniel; Brock, Andrew; Shelhamer, Evan; Hénaff, Olivier (2021년 8월 2일). “Perceiver IO: A General Architecture for Structured Inputs & Outputs”. arXiv:2107.14795 [cs.LG]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
108. ↑ “Parti: Pathways Autoregressive Text-to-Image Model”. 《sites.research.google》. 2024년 8월 9일에 확인함.
109. ↑ 가 나 Villegas, Ruben; Babaeizadeh, Mohammad; Kindermans, Pieter-Jan; Moraldo, Hernan; Zhang, Han; Saffar, Mohammad Taghi; Castro, Santiago; Kunze, Julius; Erhan, Dumitru (2022년 9월 29일). 《Phenaki: Variable Length Video Generation from Open Domain Textual Descriptions》 (영어).
110. ↑ 가 나 Chang, Huiwen; Zhang, Han; Barber, Jarred; Maschinot, A. J.; Lezama, Jose; Jiang, Lu; Yang, Ming-Hsuan; Murphy, Kevin; Freeman, William T. (2023년 1월 2일). “Muse: Text-To-Image Generation via Masked Generative Transformers”. arXiv:2301.00704 [cs.CV]. 더 이상 지원되지 않는 변수를 사용함 (도움말)
111. ↑ Ramesh, Aditya; Pavlov, Mikhail; Goh, Gabriel; Gray, Scott; Voss, Chelsea; Radford, Alec; Chen, Mark; Sutskever, Ilya (2021년 2월 26일), 《Zero-Shot Text-to-Image Generation》, arXiv:2102.12092
112. ↑ Yu, Jiahui; Xu, Yuanzhong; Koh, Jing Yu; Luong, Thang; Baid, Gunjan; Wang, Zirui; Vasudevan, Vijay; Ku, Alexander; Yang, Yinfei (2022년 6월 21일), 《Scaling Autoregressive Models for Content-Rich Text-to-Image Generation》, arXiv:2206.10789
113. ↑ Kariampuzha, William; Alyea, Gioconda; Qu, Sue; Sanjak, Jaleal; Mathé, Ewy; Sid, Eric; Chatelaine, Haley; Yadaw, Arjun; Xu, Yanji; Zhu, Qian (2023). 《Precision information extraction for rare disease epidemiology at scale》. 《Journal of Translational Medicine》 21. 157쪽. doi:10.1186/s12967-023-04011-y. PMC 9972634. PMID 36855134.

1. ↑ 게이트 순환 유닛 (2014)은 복잡성을 더욱 줄였다.
2. ↑ RWKV 또는 상태 공간 모델과 같은 일부 아키텍처는 이 문제를 피한다.

- v
- t
- e

- 알파벳
- 역사
- 인수한 기업 목록
- 제품
- 비판 개인정보 문제
- 검열
- 이스터 에그
- Don't Be Evil

- 개인정보 문제

- 애드몹
- 애드스케이프
- 애드센스
- 구글 애즈
- 애널리틱스
- 컨트리뷰터
- 파트너
- 더블클릭
- 더블클릭 포 퍼블리셔스
- 페이 샌드

- 알로
- 알리미
- 앱 스크립트
- 듀오
- 캘린더
- Gmail
- 그룹스
- 행아웃
- 인박스
- 싱크
- 텍스트 음성 변환
- 번역
- 음역
- 보이스
- 주소록

- 어시스턴트 (렌즈)
- 크롬 안드로이드용 iOS용 크롬 웹 스토어 앱 확장 프로그램
- 클라우드 프린트
- 어스 스카이 문 마스
- 가젯
- 지보드
- 고글스
- IME 병음 일본어
- 포토
- 킵
- 뉴스와 날씨
- 나우
- 오픈리파인
- 검색
- 툴바
- 패밀리 링크

- 안드로이드용
- iOS용
- 크롬 웹 스토어
- 앱
- 확장 프로그램

- 스카이
- 문
- 마스

- 병음
- 일본어

- 계정
- 안드로이드 버전 역사 소프트웨어 개발 안드로이드 오토 구글 페이 안드로이드 TV 안드로이드 웨어
- OTP
- 바디
- 도서 도서관 프로젝트
- 카하
- 가상현실 카드보드 데이드림
- 캐스트
- 크롬캐스트
- 크롬OS 크롬빗 크롬북 크롬박스 크롬 존
- 클라우드 플랫폼 앱 엔진 빅쿼리 빅테이블 컴퓨트 엔진 클라우드 스토리지
- 콘택트 렌즈
- 커스텀 서치
- 다트
- 데이드림
- 어스 엔진
- 핏
- GFS
- 글래스
- Go
- 구글 워크스페이스(구 G 스위트) 클래스룸
- 홈 구글 홈 미니
- 잼보드
- 마켓플레이스
- 네이티브 클라이언트
- 넥서스
- 온허브
- 오픈소셜
- 픽셀
- 플레이 북 게임 무비 & TV
- 퍼블릭 DNS
- 페이 샌드
- 와이파이

- 버전 역사
- 소프트웨어 개발
- 안드로이드 오토
- 구글 페이
- 안드로이드 TV
- 안드로이드 웨어

- 도서관 프로젝트

- 카드보드
- 데이드림

- 크롬빗
- 크롬북
- 크롬박스
- 크롬 존

- 앱 엔진
- 빅쿼리
- 빅테이블
- 컴퓨트 엔진
- 클라우드 스토리지

- 클래스룸

- 구글 홈 미니

- 북
- 게임
- 무비 & TV

- AJAX API
- 앱 인벤터
- 클로저 툴스
- 디벨로퍼스
- GData
- 구글봇
- 구아바
- 주스
- GWS
- KML
- 카이스
- 맵리듀스
- 미디어봇
- 사이트맵스
- 서머 오브 코드
- 웹 툴킷
- 검색 콘솔
- 웹사이트 옵티마이저
- 스위피

- 블로거
- 북마크
- 드라이브 문서, 시트, 슬라이드, 폼 드로잉 퓨전 테이블
- 도메인
- 피드버너
- 맵 메이커
- 사이트
- 유튜브
- 유튜브 인스턴트
- 유튜브 프리미엄
- VEVO
- 저갯 서베이

- 문서, 시트, 슬라이드, 폼
- 드로잉
- 퓨전 테이블

- 어플라이언스
- 블로그 검색
- 도서 엔그램 뷰어
- 커스텀 검색
- 파이낸스
- 항공편
- 이미지
- 맵스 내 지도 마스 문 스카이 스트리트 뷰 적용 범위 경쟁 서비스 개인정보 문제
- 뉴스 아카이브
- 특허
- 퍼블릭 데이터
- 학술 검색
- 쇼핑
- 유즈넷
- 비디오

- 엔그램 뷰어

- 내 지도
- 마스
- 문
- 스카이
- 스트리트 뷰 적용 범위 경쟁 서비스 개인정보 문제

- 적용 범위
- 경쟁 서비스
- 개인정보 문제

- 아카이브

- 페이지랭크
- 판다
- 펭귄
- 허밍버드

- 웹 히스토리
- 개인화 검색
- 실시간
- 순간 검색
- 세이프서치
- 음성 검색

- 인사이츠 포 서치
- 트렌드
- 지식 그래프
- 지식 금고

- Aardvark
- 앤서
- 브라우저 싱크
- 베이스
- 버즈
- 체크아웃
- 크롬 프레임
- 클릭 투 콜
- 클라우드 커넥트
- 코드 검색
- 커런츠
- 데스크톱
- 사전
- 디렉터리
- 닷지볼
- 패스트 플립
- 프렌드 커넥트
- 기어스
- GOOG-411
- 구글 TV
- 자이쿠
- 놀
- 헬스
- IGoogle
- 이미지 레이블러
- 랩스
- 래티튜드
- 라이블리
- 매시업 에디터
- 노트북
- 오퍼스
- 오르컷
- 팩
- 페이지 크리에이터
- 파노라미오
- 피카사
- 피카사 웹 앨범
- Picnik
- 파워미터
- Q & A
- 리더
- 서치위키
- 사이드위키
- 스퀘어드
- 토크
- 업데이터
- Urchin
- 비디오
- 웨이브
- 웹 가속기
- 뮤직
- 뉴스스탠드
- 구글+

- 앨 고어
- 앨런 유스터스
- 앨런 멀러리
- 아밋 싱할
- 앤 매더
- 데이비드 드러먼드
- 에릭 슈밋
- 제프 딘
- 존 도어
- 존 L. 헤네시
- 크리시나 바랏
- 맷 커츠
- 패트릭 피체트
- 폴 오텔리니
- 오미드 코데스타니
- 레이첼 웨트스톤
- 라젠 셰스
- 램 슈리램
- 레이 커즈와일
- 루스 포랏
- 살라 카만가
- 셜리 M. 틸먼
- 선다 피차이
- 수전 워치츠키
- 우르스 회즐
- 빈트 서프
- 할 배리언

- 래리 페이지
- 세르게이 브린

- 아트 앤드 컬처
- 캘리코
- 커런트
- 크롬 실험
- 코드인
- 코드 잼
- 개발자의 날
- 구글 비즈니스 그룹
- 메이드 위드 코드
- 데이터 리버레이션 테이크아웃
- 구글 디밸로퍼 엑스퍼트
- 구글 포 워크
- 무인 자동차
- 어스 아웃리치
- 파이버
- GV
- "구글"
- 구글 차이나
- 구글 익스프레스
- 구글라이제이션
- 그랜츠
- Google.org
- 루나 X 프라이즈
- 프로젝트 Fi
- 머티리얼 디자인
- 모토로라 모빌리티
- 와이파이
- X 디벨롭먼트

- 테이크아웃

- 사이언스 페어
- 서치올로지
- I/O
- 개발자의 날
- Talks at Google
- 코드 잼
- 코드인

- 아라
- 룬
- 탱고
- 선루프

- 111 Eighth Avenue
- 구글플렉스

- Doodle4Google
- 구글 두들

- AI 챌린지
- 폭탄
- 구제
- 모노폴리 시티 스트리트
- 유니티

- 분류
- 웹사이트: google.com
- google.co.kr