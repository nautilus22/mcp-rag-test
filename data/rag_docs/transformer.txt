제목: 트랜스포머 (기계 학습)

트랜스포머 (기계 학습)

트랜스포머(transformer)는 멀티 헤드 어텐션 메커니즘을 기반으로 하는 딥 러닝 아키텍처이며, 텍스트가 토큰이라는 수치 표현으로 변환되고 각 토큰은 워드 임베딩 테이블에서 조회를 통해 벡터로 변환된다. 각 레이어에서 각 토큰은 병렬 멀티 헤드 어텐션 메커니즘을 통해 컨텍스트 창 범위 내에서 다른(마스크되지 않은) 토큰과 함께 컨텍스트화되며, 이를 통해 핵심 토큰에 대한 신호가 증폭되고 덜 중요한 토큰은 약화된다.

트랜스포머는 순환 유닛이 없다는 장점이 있어 장단기 메모리 (LSTM)와 같은 이전 순환 신경망 아키텍처 (RNN)보다 훈련 시간이 적게 걸린다. 이후의 변형들은 대규모 (언어) 데이터셋에서 대형 언어 모델 (LLM)을 훈련하는 데 널리 채택되었다.

트랜스포머의 현대 버전은 2017년 구글 연구원들의 논문 어텐션 이즈 올 유 니드에서 제안되었다. 트랜스포머는 원래 기계 번역을 위한 이전 아키텍처를 개선하기 위해 개발되었지만, 이후 많은 응용 분야에서 사용되었다. 대규모 자연어 처리, 컴퓨터 비전 (비전 트랜스포머), 강화 학습, 오디오, 멀티모덜 학습, 로봇공학, 심지어 컴퓨터 체스를 두는 데에도 사용된다. 또한 사전 훈련 시스템인 GPT (Generative Pre-trained Transformers) 및 BERT (Bidirectional Encoder Representations from Transformers)의 개발로 이어졌다.